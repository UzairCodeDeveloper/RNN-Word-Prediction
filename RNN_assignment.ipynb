{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8d4e34-d3f8-4c24-b012-db0a5c3fb791",
   "metadata": {},
   "source": [
    "# **Q#1 - Single Neuron + Q3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2106922-1b1d-4f7c-a64b-e218ed747269",
   "metadata": {},
   "source": [
    "## Next Character Prediction in a Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de6c02-8345-4bb9-b841-9be02ddbf05d",
   "metadata": {},
   "source": [
    "### Example-1: welcome\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab1b7c7e-4717-4b65-9348-2fff1b2ed9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens, Indices, and Encodings:\n",
      "Character: w, Index: 5, Encoding: [0, 0, 0, 0, 0, 1]\n",
      "Character: e, Index: 1, Encoding: [0, 1, 0, 0, 0, 0]\n",
      "Character: l, Index: 2, Encoding: [0, 0, 1, 0, 0, 0]\n",
      "Character: c, Index: 0, Encoding: [1, 0, 0, 0, 0, 0]\n",
      "Character: o, Index: 4, Encoding: [0, 0, 0, 0, 1, 0]\n",
      "Character: m, Index: 3, Encoding: [0, 0, 0, 1, 0, 0]\n",
      "Character: e, Index: 1, Encoding: [0, 1, 0, 0, 0, 0]\n",
      "\n",
      "Epoch 0, Loss: [10.90636248]\n",
      "Epoch 200, Loss: [4.05437958]\n",
      "Epoch 400, Loss: [3.14399994]\n",
      "Epoch 600, Loss: [2.70887058]\n",
      "Epoch 800, Loss: [2.44174864]\n",
      "Epoch 1000, Loss: [2.26341922]\n",
      "Epoch 1200, Loss: [2.13756839]\n",
      "Epoch 1400, Loss: [2.04487734]\n",
      "Epoch 1600, Loss: [1.97422425]\n",
      "Epoch 1800, Loss: [1.91883462]\n",
      "\n",
      "Predictions with Intermediate Details:\n",
      "Input Character: w, One-Hot Encoding: [[0. 0. 0. 0. 0. 1.]]\n",
      "Predicted Probabilities: [[7.12409435e-12 9.75144124e-01 9.54147639e-06 2.48462831e-02\n",
      "  4.64022947e-12 5.10080011e-08]]\n",
      "Predicted Next Character: e\n",
      "\n",
      "Input Character: e, One-Hot Encoding: [[0. 1. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[3.32040171e-02 8.16630833e-06 8.68138992e-01 6.78011506e-02\n",
      "  3.04982587e-02 3.49415371e-04]]\n",
      "Predicted Next Character: l\n",
      "\n",
      "Input Character: l, One-Hot Encoding: [[0. 0. 1. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[4.59965738e-01 6.50322124e-13 2.80709293e-02 6.50512435e-06\n",
      "  5.11954165e-01 2.66242374e-06]]\n",
      "Predicted Next Character: o\n",
      "\n",
      "Input Character: c, One-Hot Encoding: [[1. 0. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[4.60015671e-01 6.31815463e-13 2.78153382e-02 6.38885065e-06\n",
      "  5.12159969e-01 2.63236440e-06]]\n",
      "Predicted Next Character: o\n",
      "\n",
      "Input Character: o, One-Hot Encoding: [[0. 0. 0. 0. 1. 0.]]\n",
      "Predicted Probabilities: [[1.21685771e-05 5.50129505e-02 6.64925390e-02 8.78377207e-01\n",
      "  9.43591233e-06 9.56986795e-05]]\n",
      "Predicted Next Character: m\n",
      "\n",
      "Input Character: m, One-Hot Encoding: [[0. 0. 0. 1. 0. 0.]]\n",
      "Predicted Probabilities: [[6.57730660e-12 9.75864346e-01 9.03884991e-06 2.41265661e-02\n",
      "  4.28058910e-12 4.86185797e-08]]\n",
      "Predicted Next Character: e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "word = \"welcome\"\n",
    "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
    "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
    "encoded_word = [char_to_int[ch] for ch in word]\n",
    "\n",
    "print(\"Tokens, Indices, and Encodings:\")\n",
    "for ch in word:\n",
    "    print(f\"Character: {ch}, Index: {char_to_int[ch]}, Encoding: {[1 if i == char_to_int[ch] else 0 for i in range(len(char_to_int))]}\")\n",
    "print()\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 1  # Single neuron\n",
    "input_size = len(char_to_int)\n",
    "output_size = len(char_to_int)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Model parameters\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN function\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "\n",
    "    for t in range(len(encoded_word) - 1):\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_word[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next characters\n",
    "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "print(\"\\nPredictions with Intermediate Details:\")\n",
    "for t in range(len(encoded_word) - 1):\n",
    "    x_t = np.zeros((input_size, 1))\n",
    "    x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "    print(f\"Input Character: {int_to_char[encoded_word[t]]}, One-Hot Encoding: {x_t.T}\")\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
    "    print(f\"Predicted Probabilities: {y_pred_softmax.T}\")\n",
    "    next_char = int_to_char[np.argmax(y_pred)]\n",
    "    print(f\"Predicted Next Character: {next_char}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7930c64-22c8-4dfd-929e-67ca682035e9",
   "metadata": {},
   "source": [
    "### Example-2: game\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6258676-5cb1-48bc-8db7-ff13c09ab363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens, Indices, and Encodings:\n",
      "Character: g, Index: 2, Encoding: [0, 0, 1, 0]\n",
      "Character: a, Index: 0, Encoding: [1, 0, 0, 0]\n",
      "Character: m, Index: 3, Encoding: [0, 0, 0, 1]\n",
      "Character: e, Index: 1, Encoding: [0, 1, 0, 0]\n",
      "\n",
      "Epoch 0, Loss: [4.23544279]\n",
      "Epoch 200, Loss: [0.68422389]\n",
      "Epoch 400, Loss: [0.30627913]\n",
      "Epoch 600, Loss: [0.19138866]\n",
      "Epoch 800, Loss: [0.13782205]\n",
      "Epoch 1000, Loss: [0.1072333]\n",
      "Epoch 1200, Loss: [0.08756629]\n",
      "Epoch 1400, Loss: [0.07390134]\n",
      "Epoch 1600, Loss: [0.06387382]\n",
      "Epoch 1800, Loss: [0.05621145]\n",
      "\n",
      "Predictions with Intermediate Details:\n",
      "Input Character: g, One-Hot Encoding: [[0. 0. 1. 0.]]\n",
      "Predicted Probabilities: [[0.97314607 0.01294657 0.00120573 0.01270164]]\n",
      "Predicted Next Character: a\n",
      "\n",
      "Input Character: a, One-Hot Encoding: [[1. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[1.12253153e-02 1.87673588e-08 1.34186711e-05 9.88761247e-01]]\n",
      "Predicted Next Character: m\n",
      "\n",
      "Input Character: m, One-Hot Encoding: [[0. 0. 0. 1.]]\n",
      "Predicted Probabilities: [[1.14553035e-02 9.88529971e-01 1.46988590e-05 2.70778322e-08]]\n",
      "Predicted Next Character: e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "word = \"game\"\n",
    "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
    "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
    "encoded_word = [char_to_int[ch] for ch in word]\n",
    "\n",
    "print(\"Tokens, Indices, and Encodings:\")\n",
    "for ch in word:\n",
    "    print(f\"Character: {ch}, Index: {char_to_int[ch]}, Encoding: {[1 if i == char_to_int[ch] else 0 for i in range(len(char_to_int))]}\")\n",
    "print()\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 1  # Single neuron\n",
    "input_size = len(char_to_int)\n",
    "output_size = len(char_to_int)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Model parameters\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN function\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "\n",
    "    for t in range(len(encoded_word) - 1):\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_word[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next characters\n",
    "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "print(\"\\nPredictions with Intermediate Details:\")\n",
    "for t in range(len(encoded_word) - 1):\n",
    "    x_t = np.zeros((input_size, 1))\n",
    "    x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "    print(f\"Input Character: {int_to_char[encoded_word[t]]}, One-Hot Encoding: {x_t.T}\")\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
    "    print(f\"Predicted Probabilities: {y_pred_softmax.T}\")\n",
    "    next_char = int_to_char[np.argmax(y_pred)]\n",
    "    print(f\"Predicted Next Character: {next_char}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1cb4e-f07c-4f70-b87c-b7ab6d5b3bb5",
   "metadata": {},
   "source": [
    "## Next Word Prediction in a Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b389458-8cc2-46d1-a9ac-d1de67a28fb5",
   "metadata": {},
   "source": [
    "### Example-1 : He is playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca90632c-4de4-46a8-8e98-9039ec20033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Tokens, Indices, and One-Hot Encodings:\n",
      "Word: He, Index: 0, One-Hot Encoding: [1. 0. 0.]\n",
      "Word: is, Index: 1, One-Hot Encoding: [0. 1. 0.]\n",
      "Word: playing, Index: 2, One-Hot Encoding: [0. 0. 1.]\n",
      "\n",
      "Epoch 0, Loss: [2.23216931]\n",
      "Epoch 200, Loss: [0.06741978]\n",
      "Epoch 400, Loss: [0.024272]\n",
      "Epoch 600, Loss: [0.01468419]\n",
      "Epoch 800, Loss: [0.01050401]\n",
      "Epoch 1000, Loss: [0.00816874]\n",
      "Epoch 1200, Loss: [0.0066795]\n",
      "Epoch 1400, Loss: [0.00564768]\n",
      "Epoch 1600, Loss: [0.0048909]\n",
      "Epoch 1800, Loss: [0.00431226]\n",
      "\n",
      "Predictions:\n",
      "Input Word: He, One-Hot Encoding: [1. 0. 0.]\n",
      "Predicted Probabilities: [6.53311714e-04 9.98102463e-01 1.24422566e-03]\n",
      "Predicted Next Word: is\n",
      "\n",
      "Input Word: is, One-Hot Encoding: [0. 1. 0.]\n",
      "Predicted Probabilities: [6.88516387e-04 1.26452396e-03 9.98046960e-01]\n",
      "Predicted Next Word: playing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "sentence = \"He is playing\"\n",
    "words = sentence.split()\n",
    "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "encoded_sentence = [word_to_int[word] for word in words]\n",
    "\n",
    "\n",
    "# Display tokens, indices, and one-hot encodings\n",
    "print(\"\\nSentence Tokens, Indices, and One-Hot Encodings:\")\n",
    "for word, idx in word_to_int.items():\n",
    "    one_hot = np.zeros((input_size, 1))\n",
    "    one_hot[idx] = 1\n",
    "    print(f\"Word: {word}, Index: {idx}, One-Hot Encoding: {one_hot.flatten()}\")\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(word_to_int)  # Number of unique words\n",
    "output_size = len(word_to_int)\n",
    "hidden_size = 1  # Single neuron\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN forward step\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "\n",
    "    for t in range(len(encoded_sentence) - 1):\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_sentence[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass (gradient calculation and parameter update)\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next words\n",
    "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "print(\"\\nPredictions:\")\n",
    "for t in range(len(encoded_sentence) - 1):\n",
    "    x_t = np.zeros((input_size, 1))\n",
    "    x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "    next_word = int_to_word[np.argmax(y_pred)]\n",
    "    print(f\"Input Word: {int_to_word[encoded_sentence[t]]}, One-Hot Encoding: {x_t.flatten()}\")\n",
    "    print(f\"Predicted Probabilities: {y_pred_softmax.flatten()}\")\n",
    "    print(f\"Predicted Next Word: {next_word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e907570-f6d8-4e46-8233-4d484f031e10",
   "metadata": {},
   "source": [
    "### Example-2 : Comeback kyu nhi ho rha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4193b264-cf87-43eb-a86f-52a129738158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Tokens, Indices, and One-Hot Encodings:\n",
      "Word: Comeback, Index: 0, One-Hot Encoding: [1. 0. 0. 0. 0.]\n",
      "Word: ho, Index: 1, One-Hot Encoding: [0. 1. 0. 0. 0.]\n",
      "Word: kyu, Index: 2, One-Hot Encoding: [0. 0. 1. 0. 0.]\n",
      "Word: nhi, Index: 3, One-Hot Encoding: [0. 0. 0. 1. 0.]\n",
      "Word: rha, Index: 4, One-Hot Encoding: [0. 0. 0. 0. 1.]\n",
      "\n",
      "Epoch 0, Loss: [6.55931436]\n",
      "Epoch 200, Loss: [2.28265962]\n",
      "Epoch 400, Loss: [1.58476885]\n",
      "Epoch 600, Loss: [1.00633306]\n",
      "Epoch 800, Loss: [0.74258773]\n",
      "Epoch 1000, Loss: [0.58800912]\n",
      "Epoch 1200, Loss: [0.48457114]\n",
      "Epoch 1400, Loss: [0.41071694]\n",
      "Epoch 1600, Loss: [0.35562322]\n",
      "Epoch 1800, Loss: [0.31333208]\n",
      "\n",
      "Predictions:\n",
      "Input Word: Comeback, One-Hot Encoding: [1. 0. 0. 0. 0.]\n",
      "Predicted Probabilities: [8.65462749e-07 6.84211428e-06 9.59846925e-01 4.01453672e-02\n",
      " 1.32773047e-12]\n",
      "Predicted Next Word: kyu\n",
      "\n",
      "Input Word: kyu, One-Hot Encoding: [0. 0. 1. 0. 0.]\n",
      "Predicted Probabilities: [2.93223314e-04 4.85944771e-02 4.49441171e-02 9.06163671e-01\n",
      " 4.51156011e-06]\n",
      "Predicted Next Word: nhi\n",
      "\n",
      "Input Word: nhi, One-Hot Encoding: [0. 0. 0. 1. 0.]\n",
      "Predicted Probabilities: [2.45610725e-04 9.06329115e-01 4.36225066e-06 4.79236048e-02\n",
      " 4.54973072e-02]\n",
      "Predicted Next Word: ho\n",
      "\n",
      "Input Word: ho, One-Hot Encoding: [0. 1. 0. 0. 0.]\n",
      "Predicted Probabilities: [5.03966460e-07 3.93165876e-02 1.20673274e-12 6.50190706e-06\n",
      " 9.60676407e-01]\n",
      "Predicted Next Word: rha\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "sentence = \"Comeback kyu nhi ho rha\"\n",
    "words = sentence.split()\n",
    "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "encoded_sentence = [word_to_int[word] for word in words]\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(word_to_int)  # Number of unique words\n",
    "output_size = len(word_to_int)\n",
    "hidden_size = 1  # Single neuron\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Display tokens, indices, and one-hot encodings\n",
    "print(\"\\nSentence Tokens, Indices, and One-Hot Encodings:\")\n",
    "for word, idx in word_to_int.items():\n",
    "    one_hot = np.zeros((input_size, 1))\n",
    "    one_hot[idx] = 1\n",
    "    print(f\"Word: {word}, Index: {idx}, One-Hot Encoding: {one_hot.flatten()}\")\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN forward step\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "\n",
    "    for t in range(len(encoded_sentence) - 1):\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_sentence[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass (gradient calculation and parameter update)\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next words\n",
    "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "print(\"\\nPredictions:\")\n",
    "for t in range(len(encoded_sentence) - 1):\n",
    "    x_t = np.zeros((input_size, 1))\n",
    "    x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "    next_word = int_to_word[np.argmax(y_pred)]\n",
    "    print(f\"Input Word: {int_to_word[encoded_sentence[t]]}, One-Hot Encoding: {x_t.flatten()}\")\n",
    "    print(f\"Predicted Probabilities: {y_pred_softmax.flatten()}\")\n",
    "    print(f\"Predicted Next Word: {next_word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11872af-b2c7-4ae6-8533-66c0cc5b05b0",
   "metadata": {},
   "source": [
    "# **Q#2 - Multiple Neurons + Q3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375610a-1e3d-44c0-ae61-3ff9b9cf4a87",
   "metadata": {},
   "source": [
    "## Next Character Prediction in a Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb93be3-6f45-463e-bab3-0cad7aed1152",
   "metadata": {},
   "source": [
    "### Example-1 : DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "530b4cc1-ac19-48e5-bf6a-27df2350c8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens, Indices, and Encodings:\n",
      "Character: d, Index: 0, Encoding: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Character: e, Index: 1, Encoding: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Character: v, Index: 8, Encoding: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "Character: e, Index: 1, Encoding: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Character: l, Index: 2, Encoding: [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Character: o, Index: 5, Encoding: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Character: p, Index: 6, Encoding: [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "Character: m, Index: 3, Encoding: [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Character: e, Index: 1, Encoding: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Character: n, Index: 4, Encoding: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "Character: t, Index: 7, Encoding: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "Epoch 0, Loss: [22.18960372]\n",
      "Epoch 200, Loss: [0.23934916]\n",
      "Epoch 400, Loss: [0.26349133]\n",
      "Epoch 600, Loss: [0.10502653]\n",
      "Epoch 800, Loss: [0.06242538]\n",
      "Epoch 1000, Loss: [0.04277837]\n",
      "Epoch 1200, Loss: [0.03195629]\n",
      "Epoch 1400, Loss: [0.02595506]\n",
      "Epoch 1600, Loss: [0.02208076]\n",
      "Epoch 1800, Loss: [0.01922875]\n",
      "\n",
      "Predictions with Intermediate Details:\n",
      "Input Character: d, One-Hot Encoding: [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[4.17741915e-06 9.99839578e-01 1.30307462e-07 1.36200988e-05\n",
      "  9.34292246e-06 3.31551287e-07 6.08805194e-06 1.01008683e-04\n",
      "  2.57229231e-05]]\n",
      "Predicted Next Character: e\n",
      "\n",
      "Input Character: e, One-Hot Encoding: [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[3.27144660e-06 4.05211204e-04 2.41732957e-04 1.50790095e-07\n",
      "  4.51210317e-04 1.11222050e-06 1.27155094e-08 1.45082239e-04\n",
      "  9.98752216e-01]]\n",
      "Predicted Next Character: v\n",
      "\n",
      "Input Character: v, One-Hot Encoding: [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Predicted Probabilities: [[1.35586712e-05 9.98188743e-01 3.41388471e-04 1.68282827e-07\n",
      "  2.26613797e-06 2.65439155e-04 7.44270041e-05 6.15009822e-04\n",
      "  4.98999785e-04]]\n",
      "Predicted Next Character: e\n",
      "\n",
      "Input Character: e, One-Hot Encoding: [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[2.73158756e-05 4.72049060e-04 9.96584334e-01 1.25338937e-05\n",
      "  1.29049217e-06 1.96261684e-03 5.18694123e-04 2.42809804e-04\n",
      "  1.78356011e-04]]\n",
      "Predicted Next Character: l\n",
      "\n",
      "Input Character: l, One-Hot Encoding: [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[9.17564059e-06 2.98066646e-05 2.41738081e-03 4.09643455e-05\n",
      "  2.36642252e-07 9.97198013e-01 2.62307321e-04 2.79489872e-06\n",
      "  3.93206202e-05]]\n",
      "Predicted Next Character: o\n",
      "\n",
      "Input Character: o, One-Hot Encoding: [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[4.91400909e-06 3.34598625e-06 8.85010332e-06 6.65109941e-04\n",
      "  2.02621669e-05 6.58217282e-04 9.98616402e-01 2.28976460e-05\n",
      "  5.55637299e-10]]\n",
      "Predicted Next Character: p\n",
      "\n",
      "Input Character: p, One-Hot Encoding: [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "Predicted Probabilities: [[8.19963673e-06 1.69943231e-04 2.08281400e-08 9.98766805e-01\n",
      "  1.08865538e-05 7.90838339e-05 9.45162385e-04 1.98313764e-05\n",
      "  6.75122157e-08]]\n",
      "Predicted Next Character: m\n",
      "\n",
      "Input Character: m, One-Hot Encoding: [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[5.55861798e-06 9.98778986e-01 1.21072830e-10 6.23212097e-04\n",
      "  3.46171133e-04 1.94771038e-07 3.46578380e-05 2.08365786e-04\n",
      "  2.85330556e-06]]\n",
      "Predicted Next Character: e\n",
      "\n",
      "Input Character: e, One-Hot Encoding: [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[5.07545208e-06 3.77049835e-05 2.08510806e-06 2.17830011e-06\n",
      "  9.98237836e-01 2.57245965e-09 2.47552628e-05 1.22819517e-03\n",
      "  4.62167508e-04]]\n",
      "Predicted Next Character: n\n",
      "\n",
      "Input Character: n, One-Hot Encoding: [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[5.17777472e-06 1.05855067e-03 1.23334536e-06 6.89835543e-07\n",
      "  5.38401927e-04 1.06325127e-08 2.19768662e-05 9.98084339e-01\n",
      "  2.89620388e-04]]\n",
      "Predicted Next Character: t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "word = \"development\"\n",
    "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
    "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
    "encoded_word = [char_to_int[ch] for ch in word]\n",
    "\n",
    "print(\"Tokens, Indices, and Encodings:\")\n",
    "for ch in word:\n",
    "    print(f\"Character: {ch}, Index: {char_to_int[ch]}, Encoding: {[1 if i == char_to_int[ch] else 0 for i in range(len(char_to_int))]}\")\n",
    "print()\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 8 # Mulit Neurons\n",
    "input_size = len(char_to_int)\n",
    "output_size = len(char_to_int)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Model parameters\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN function\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "\n",
    "    for t in range(len(encoded_word) - 1):\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_word[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next characters\n",
    "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "print(\"\\nPredictions with Intermediate Details:\")\n",
    "for t in range(len(encoded_word) - 1):\n",
    "    x_t = np.zeros((input_size, 1))\n",
    "    x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "    print(f\"Input Character: {int_to_char[encoded_word[t]]}, One-Hot Encoding: {x_t.T}\")\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
    "    print(f\"Predicted Probabilities: {y_pred_softmax.T}\")\n",
    "    next_char = int_to_char[np.argmax(y_pred)]\n",
    "    print(f\"Predicted Next Character: {next_char}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb5dd4-3670-457c-bac3-94810ebd3760",
   "metadata": {},
   "source": [
    "### Example-2 : CRICKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0430ebd3-46c1-4ef6-bc47-955fa4653a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens, Indices, and Encodings:\n",
      "Character: C, Index: 0, Encoding: [1, 0, 0, 0, 0, 0]\n",
      "Character: R, Index: 4, Encoding: [0, 0, 0, 0, 1, 0]\n",
      "Character: I, Index: 2, Encoding: [0, 0, 1, 0, 0, 0]\n",
      "Character: C, Index: 0, Encoding: [1, 0, 0, 0, 0, 0]\n",
      "Character: K, Index: 3, Encoding: [0, 0, 0, 1, 0, 0]\n",
      "Character: E, Index: 1, Encoding: [0, 1, 0, 0, 0, 0]\n",
      "Character: T, Index: 5, Encoding: [0, 0, 0, 0, 0, 1]\n",
      "\n",
      "Epoch 0, Loss: [10.99940466]\n",
      "Epoch 200, Loss: [0.09080183]\n",
      "Epoch 400, Loss: [0.03648428]\n",
      "Epoch 600, Loss: [0.02293228]\n",
      "Epoch 800, Loss: [0.01672634]\n",
      "Epoch 1000, Loss: [0.01315143]\n",
      "Epoch 1200, Loss: [0.01082475]\n",
      "Epoch 1400, Loss: [0.00919048]\n",
      "Epoch 1600, Loss: [0.0079805]\n",
      "Epoch 1800, Loss: [0.00704922]\n",
      "\n",
      "Predictions with Intermediate Details:\n",
      "Input Character: C, One-Hot Encoding: [[1. 0. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[1.68764256e-06 3.25482275e-04 6.20125845e-04 3.29998651e-04\n",
      "  9.98460143e-01 2.62562390e-04]]\n",
      "Predicted Next Character: R\n",
      "\n",
      "Input Character: R, One-Hot Encoding: [[0. 0. 0. 0. 1. 0.]]\n",
      "Predicted Probabilities: [[1.71671901e-04 1.14624077e-05 9.99002824e-01 3.45082303e-08\n",
      "  7.19484553e-04 9.45224951e-05]]\n",
      "Predicted Next Character: I\n",
      "\n",
      "Input Character: I, One-Hot Encoding: [[0. 0. 1. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[9.98807706e-01 6.26434234e-06 4.57171053e-04 6.64308052e-04\n",
      "  1.15840078e-06 6.33924736e-05]]\n",
      "Predicted Next Character: C\n",
      "\n",
      "Input Character: C, One-Hot Encoding: [[1. 0. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[4.49112223e-04 2.46063846e-04 5.35285280e-08 9.98973731e-01\n",
      "  3.27778264e-04 3.26137005e-06]]\n",
      "Predicted Next Character: K\n",
      "\n",
      "Input Character: K, One-Hot Encoding: [[0. 0. 0. 1. 0. 0.]]\n",
      "Predicted Probabilities: [[1.63033797e-06 9.99148922e-01 1.62815330e-07 4.94061224e-04\n",
      "  2.65012132e-04 9.02110979e-05]]\n",
      "Predicted Next Character: E\n",
      "\n",
      "Input Character: E, One-Hot Encoding: [[0. 1. 0. 0. 0. 0.]]\n",
      "Predicted Probabilities: [[3.51268461e-05 2.05647064e-04 2.00403886e-05 5.34963773e-05\n",
      "  3.79538039e-04 9.99306151e-01]]\n",
      "Predicted Next Character: T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "word = \"CRICKET\"\n",
    "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
    "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
    "encoded_word = [char_to_int[ch] for ch in word]\n",
    "\n",
    "print(\"Tokens, Indices, and Encodings:\")\n",
    "for ch in word:\n",
    "    print(f\"Character: {ch}, Index: {char_to_int[ch]}, Encoding: {[1 if i == char_to_int[ch] else 0 for i in range(len(char_to_int))]}\")\n",
    "print()\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 8 # Mulit Neurons\n",
    "input_size = len(char_to_int)\n",
    "output_size = len(char_to_int)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Model parameters\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN function\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "\n",
    "    for t in range(len(encoded_word) - 1):\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_word[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next characters\n",
    "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "print(\"\\nPredictions with Intermediate Details:\")\n",
    "for t in range(len(encoded_word) - 1):\n",
    "    x_t = np.zeros((input_size, 1))\n",
    "    x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "    print(f\"Input Character: {int_to_char[encoded_word[t]]}, One-Hot Encoding: {x_t.T}\")\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
    "    print(f\"Predicted Probabilities: {y_pred_softmax.T}\")\n",
    "    next_char = int_to_char[np.argmax(y_pred)]\n",
    "    print(f\"Predicted Next Character: {next_char}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d579e-4c2e-447e-a61a-1935d27705d3",
   "metadata": {},
   "source": [
    "## Next Word Prediction in a Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc0809-e58f-4a0a-9ce7-ce7ef3d0f716",
   "metadata": {},
   "source": [
    "### Example-1 I love NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a86fba2d-4890-49da-9c4b-733b93804b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Tokens, Indices, and One-Hot Encodings:\n",
      "Word: I, Index: 0, One-Hot Encoding: [1. 0. 0.]\n",
      "Word: NN, Index: 1, One-Hot Encoding: [0. 1. 0.]\n",
      "Word: love, Index: 2, One-Hot Encoding: [0. 0. 1.]\n",
      "\n",
      "Epoch 0, Loss: [2.23166157]\n",
      "Epoch 200, Loss: [0.05183321]\n",
      "Epoch 400, Loss: [0.02155832]\n",
      "Epoch 600, Loss: [0.01346595]\n",
      "Epoch 800, Loss: [0.00973809]\n",
      "Epoch 1000, Loss: [0.00759989]\n",
      "Epoch 1200, Loss: [0.00621575]\n",
      "Epoch 1400, Loss: [0.00524786]\n",
      "Epoch 1600, Loss: [0.00453374]\n",
      "Epoch 1800, Loss: [0.00398563]\n",
      "\n",
      "Predictions:\n",
      "Input Word: I, One-Hot Encoding: [1. 0. 0.]\n",
      "Predicted Probabilities: [4.29913128e-04 9.29071971e-04 9.98641015e-01]\n",
      "Predicted Next Word: love\n",
      "\n",
      "Input Word: love, One-Hot Encoding: [0. 0. 1.]\n",
      "Predicted Probabilities: [6.79367656e-04 9.97818496e-01 1.50213637e-03]\n",
      "Predicted Next Word: NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "sentence = \"I love NN\"\n",
    "words = sentence.split()\n",
    "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "encoded_sentence = [word_to_int[word] for word in words]\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(word_to_int)  # Number of unique words\n",
    "output_size = len(word_to_int)\n",
    "hidden_size = 8  # Multi neurons\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Display tokens, indices, and one-hot encodings\n",
    "print(\"\\nSentence Tokens, Indices, and One-Hot Encodings:\")\n",
    "for word, idx in word_to_int.items():\n",
    "    one_hot = np.zeros((input_size, 1))\n",
    "    one_hot[idx] = 1\n",
    "    print(f\"Word: {word}, Index: {idx}, One-Hot Encoding: {one_hot.flatten()}\")\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN forward step\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "\n",
    "    for t in range(len(encoded_sentence) - 1):\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_sentence[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass (gradient calculation and parameter update)\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next words\n",
    "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "print(\"\\nPredictions:\")\n",
    "for t in range(len(encoded_sentence) - 1):\n",
    "    x_t = np.zeros((input_size, 1))\n",
    "    x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "    next_word = int_to_word[np.argmax(y_pred)]\n",
    "    print(f\"Input Word: {int_to_word[encoded_sentence[t]]}, One-Hot Encoding: {x_t.flatten()}\")\n",
    "    print(f\"Predicted Probabilities: {y_pred_softmax.flatten()}\")\n",
    "    print(f\"Predicted Next Word: {next_word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e8762-d8d7-497f-8bc4-3bec19791582",
   "metadata": {},
   "source": [
    "### Example-2 Smudge is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3bf5c3c6-935d-4d87-bb4b-9d7c82333984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Tokens, Indices, and One-Hot Encodings:\n",
      "Word: Smudge, Index: 0, One-Hot Encoding: [1. 0. 0. 0.]\n",
      "Word: best, Index: 1, One-Hot Encoding: [0. 1. 0. 0.]\n",
      "Word: is, Index: 2, One-Hot Encoding: [0. 0. 1. 0.]\n",
      "Word: the, Index: 3, One-Hot Encoding: [0. 0. 0. 1.]\n",
      "\n",
      "Epoch 0, Loss: [4.23524531]\n",
      "Epoch 200, Loss: [0.20186681]\n",
      "Epoch 400, Loss: [0.04057598]\n",
      "Epoch 600, Loss: [0.0146723]\n",
      "Epoch 800, Loss: [0.00845714]\n",
      "Epoch 1000, Loss: [0.00583428]\n",
      "Epoch 1200, Loss: [0.00441655]\n",
      "Epoch 1400, Loss: [0.00353052]\n",
      "Epoch 1600, Loss: [0.00292899]\n",
      "Epoch 1800, Loss: [0.00249745]\n",
      "\n",
      "Predictions:\n",
      "Input Word: Smudge, One-Hot Encoding: [1. 0. 0. 0.]\n",
      "Predicted Probabilities: [1.86147591e-04 4.42629534e-04 9.98976539e-01 3.94684338e-04]\n",
      "Predicted Next Word: is\n",
      "\n",
      "Input Word: is, One-Hot Encoding: [0. 0. 1. 0.]\n",
      "Predicted Probabilities: [5.80018291e-05 8.38760028e-06 7.44036458e-04 9.99189574e-01]\n",
      "Predicted Next Word: the\n",
      "\n",
      "Input Word: the, One-Hot Encoding: [0. 0. 0. 1.]\n",
      "Predicted Probabilities: [3.75814214e-05 9.99664512e-01 4.26957667e-06 2.93637062e-04]\n",
      "Predicted Next Word: best\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "sentence = \"Smudge is the best\"\n",
    "words = sentence.split()\n",
    "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "encoded_sentence = [word_to_int[word] for word in words]\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(word_to_int)  # Number of unique words\n",
    "output_size = len(word_to_int)\n",
    "hidden_size = 8  # Multi neurons\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Display tokens, indices, and one-hot encodings\n",
    "print(\"\\nSentence Tokens, Indices, and One-Hot Encodings:\")\n",
    "for word, idx in word_to_int.items():\n",
    "    one_hot = np.zeros((input_size, 1))\n",
    "    one_hot[idx] = 1\n",
    "    print(f\"Word: {word}, Index: {idx}, One-Hot Encoding: {one_hot.flatten()}\")\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN forward step\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "\n",
    "    for t in range(len(encoded_sentence) - 1):\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_sentence[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass (gradient calculation and parameter update)\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next words\n",
    "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "print(\"\\nPredictions:\")\n",
    "for t in range(len(encoded_sentence) - 1):\n",
    "    x_t = np.zeros((input_size, 1))\n",
    "    x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "    next_word = int_to_word[np.argmax(y_pred)]\n",
    "    print(f\"Input Word: {int_to_word[encoded_sentence[t]]}, One-Hot Encoding: {x_t.flatten()}\")\n",
    "    print(f\"Predicted Probabilities: {y_pred_softmax.flatten()}\")\n",
    "    print(f\"Predicted Next Word: {next_word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e838e-b8ec-4aa1-88f0-abca3e3f2fd0",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a5153-b618-42d2-9e2c-66b9f7a651bb",
   "metadata": {},
   "source": [
    "#### Next Character Prediction in Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "efd34088-1578-4cbc-ac5f-fc04decaacb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Word Embedding ###\n",
      "Sentence: I love coding\n",
      "Tokens (Words): ['I', 'love', 'coding']\n",
      "Vocabulary (Word to Index): {'I': 0, 'coding': 1, 'love': 2}\n",
      "Index to Word Mapping: {0: 'I', 1: 'coding', 2: 'love'}\n",
      "Encoded Sentence: [0, 2, 1] \n",
      "\n",
      "Epoch 0, Loss: [2.23168079]\n",
      "Epoch 200, Loss: [1.47187693]\n",
      "Epoch 400, Loss: [1.45506915]\n",
      "Epoch 600, Loss: [1.44944349]\n",
      "Epoch 800, Loss: [1.44663146]\n",
      "Epoch 1000, Loss: [1.44494573]\n",
      "Epoch 1200, Loss: [1.44382295]\n",
      "Epoch 1400, Loss: [1.44302163]\n",
      "Epoch 1600, Loss: [1.44242108]\n",
      "Epoch 1800, Loss: [1.44195428]\n",
      "\n",
      "Predictions:\n",
      "Input: I, Predicted: coding\n",
      "Input: love, Predicted: coding\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "sentence = \"I love coding\"\n",
    "words = sentence.split()\n",
    "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "encoded_sentence = [word_to_int[word] for word in words]\n",
    "\n",
    "# Display tokens, vocabulary, and encodings\n",
    "print(\"### Word Embedding ###\")\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Tokens (Words):\", words)\n",
    "print(\"Vocabulary (Word to Index):\", word_to_int)\n",
    "print(\"Index to Word Mapping:\", int_to_word)\n",
    "print(\"Encoded Sentence:\", encoded_sentence, \"\\n\")\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(word_to_int)  # Number of unique words\n",
    "embedding_dim = 3  # Size of word embeddings\n",
    "hidden_size = 1  # Single neuron\n",
    "output_size = vocab_size\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize weights and biases\n",
    "embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.01  # Word embeddings\n",
    "Wx = np.random.randn(hidden_size, embedding_dim) * 0.01  # Embedding to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN forward step\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "\n",
    "    for t in range(len(encoded_sentence) - 1):\n",
    "        word_idx = encoded_sentence[t]\n",
    "        x_t = embedding_matrix[word_idx].reshape(-1, 1)  # Word embedding vector\n",
    "        y_true = encoded_sentence[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass (gradient calculation and parameter update)\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next words\n",
    "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "print(\"\\nPredictions:\")\n",
    "for t in range(len(encoded_sentence) - 1):\n",
    "    word_idx = encoded_sentence[t]\n",
    "    x_t = embedding_matrix[word_idx].reshape(-1, 1)  # Word embedding vector\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    next_word = int_to_word[np.argmax(y_pred)]\n",
    "    print(f\"Input: {int_to_word[encoded_sentence[t]]}, Predicted: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a04b57-095b-4878-894e-8568b1298e71",
   "metadata": {},
   "source": [
    "### Next Word Prediction in a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "084da036-4c6e-4db2-91e7-dc764d0dc671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding\n",
      "Sentence: I love coding\n",
      "Tokens (Words): ['I', 'love', 'coding']\n",
      "Vocabulary (Word to Index): {'I': 0, 'coding': 1, 'love': 2}\n",
      "Index to Word Mapping: {0: 'I', 1: 'coding', 2: 'love'}\n",
      "Encoded Sentence: [0, 2, 1] \n",
      "\n",
      "Epoch 0, Loss: [2.23167541]\n",
      "Epoch 200, Loss: [1.47197037]\n",
      "Epoch 400, Loss: [1.4552161]\n",
      "Epoch 600, Loss: [1.44956189]\n",
      "Epoch 800, Loss: [1.44671776]\n",
      "Epoch 1000, Loss: [1.44500834]\n",
      "Epoch 1200, Loss: [1.44386867]\n",
      "Epoch 1400, Loss: [1.44305467]\n",
      "Epoch 1600, Loss: [1.44244376]\n",
      "Epoch 1800, Loss: [1.44196761]\n",
      "\n",
      "Predictions:\n",
      "Input: I, Predicted: coding\n",
      "Input: love, Predicted: coding\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "sentence = \"I love coding\"\n",
    "words = sentence.split()\n",
    "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "encoded_sentence = [word_to_int[word] for word in words]\n",
    "\n",
    "# Display tokens, vocabulary, and encodings\n",
    "print(\"Word Embedding\")\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Tokens (Words):\", words)\n",
    "print(\"Vocabulary (Word to Index):\", word_to_int)\n",
    "print(\"Index to Word Mapping:\", int_to_word)\n",
    "print(\"Encoded Sentence:\", encoded_sentence, \"\\n\")\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(word_to_int)  # Number of unique words\n",
    "embedding_dim = 3  # Size of word embeddings\n",
    "hidden_size = 1  # Single neuron\n",
    "output_size = vocab_size\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize weights and biases\n",
    "embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.01  # Word embeddings\n",
    "Wx = np.random.randn(hidden_size, embedding_dim) * 0.01  # Embedding to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN forward step\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2000):  # Training for 2000 epochs\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "\n",
    "    for t in range(len(encoded_sentence) - 1):\n",
    "        word_idx = encoded_sentence[t]\n",
    "        x_t = embedding_matrix[word_idx].reshape(-1, 1)  # Word embedding vector\n",
    "        y_true = encoded_sentence[t + 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass (gradient calculation and parameter update)\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predict next words\n",
    "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "print(\"\\nPredictions:\")\n",
    "for t in range(len(encoded_sentence) - 1):\n",
    "    word_idx = encoded_sentence[t]\n",
    "    x_t = embedding_matrix[word_idx].reshape(-1, 1)  # Word embedding vector\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    next_word = int_to_word[np.argmax(y_pred)]\n",
    "    print(f\"Input: {int_to_word[encoded_sentence[t]]}, Predicted: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b2d28-65af-433f-bc2b-339f4a66ff9f",
   "metadata": {},
   "source": [
    "## **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c5bea3a1-1e25-4ff1-ad8f-d24c1d0683ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Word using Bag of Words:\n",
      " [1 1 2 1] \n",
      "\n",
      "RNN Output using Bag of Words Encoding:\n",
      " [[ 1.88214162e-04]\n",
      " [-2.66831921e-05]\n",
      " [ 3.93979274e-04]\n",
      " [-1.42594391e-05]]\n",
      "Predicted next character based on Bag of Words encoding: l\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "word = \"hello\"\n",
    "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
    "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
    "\n",
    "# Bag of Words Representation\n",
    "bag_of_words = np.zeros((len(char_to_int),), dtype=int)\n",
    "for ch in word:\n",
    "    bag_of_words[char_to_int[ch]] += 1\n",
    "\n",
    "print(\"Encoded Word using Bag of Words:\\n\", bag_of_words, \"\\n\")\n",
    "\n",
    "# Define Hyperparameters and Model Parameters\n",
    "hidden_size = 1  # Single neuron\n",
    "input_size = len(char_to_int)\n",
    "output_size = len(char_to_int)\n",
    "learning_rate = 0.1\n",
    "\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((output_size, 1))\n",
    "\n",
    "# RNN function\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Example: Input to RNN\n",
    "x_input = bag_of_words.reshape(-1, 1)  # The Bag of Words vector as input\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "h_next, y_pred = rnn_step_forward(x_input, h_prev)\n",
    "\n",
    "# Training loop (conceptual only for Bag of Words)\n",
    "# Bag of Words represents the entire word as a single input, so there's no sequential prediction\n",
    "print(\"RNN Output using Bag of Words Encoding:\\n\", y_pred)\n",
    "\n",
    "# Prediction\n",
    "predicted_index = np.argmax(y_pred)  # Choose the index with the highest probability\n",
    "predicted_char = int_to_char[predicted_index]\n",
    "print(f\"Predicted next character based on Bag of Words encoding: {predicted_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973661c7-1d5b-4d69-83de-bada18ac58f3",
   "metadata": {},
   "source": [
    "**Bag of Words - Using Sliding Window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "53e8e6d8-160c-4c60-942d-bceab05ab9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding Window Encoded Words (Bag of Words):\n",
      "Window 1: [1 1 0 0]\n",
      "Window 2: [1 0 1 0]\n",
      "Window 3: [0 0 2 0]\n",
      "Window 4: [0 0 1 1]\n",
      "Window 1: Predicted next character: h\n",
      "Window 2: Predicted next character: o\n",
      "Window 3: Predicted next character: o\n",
      "Window 4: Predicted next character: o\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "word = \"hello\"\n",
    "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
    "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
    "\n",
    "# Bag of Words with Sliding Windows\n",
    "window_size = 2  # Define the size of the sliding window\n",
    "encoded_windows = []\n",
    "\n",
    "for i in range(len(word) - window_size + 1):\n",
    "    window = word[i : i + window_size]  # Extract a window\n",
    "    bag_of_words = np.zeros((len(char_to_int),), dtype=int)\n",
    "    for ch in window:\n",
    "        bag_of_words[char_to_int[ch]] += 1\n",
    "    encoded_windows.append(bag_of_words)\n",
    "\n",
    "print(\"Sliding Window Encoded Words (Bag of Words):\")\n",
    "for i, bow in enumerate(encoded_windows):\n",
    "    print(f\"Window {i + 1}: {bow}\")\n",
    "\n",
    "# Feed these sliding windows to an RNN\n",
    "hidden_size = 1\n",
    "Wx = np.random.randn(hidden_size, len(char_to_int)) * 0.01\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Wy = np.random.randn(len(char_to_int), hidden_size) * 0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((len(char_to_int), 1))\n",
    "\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Sequentially process the sliding windows\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "for t, bow in enumerate(encoded_windows):\n",
    "    x_t = bow.reshape(-1, 1)  # Reshape for RNN input\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    next_char = int_to_char[np.argmax(y_pred)]\n",
    "    print(f\"Window {t + 1}: Predicted next character: {next_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808298d-ef19-452a-a636-9ebbaca270b6",
   "metadata": {},
   "source": [
    "## **Hashing Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2428fd53-8afe-4fe1-ac5a-887fa2c97a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Word using Hashing Encoding (character-by-character):\n",
      "Character 'h': [0 0 0 1 0]\n",
      "Character 'e': [0 1 0 0 0]\n",
      "Character 'l': [0 0 1 0 0]\n",
      "Character 'l': [0 0 1 0 0]\n",
      "Character 'o': [0 0 0 1 0]\n",
      "\n",
      "\n",
      "Predictions:\n",
      "Input: 'h', Predicted next character: 'l'\n",
      "Input: 'e', Predicted next character: 'l'\n",
      "Input: 'l', Predicted next character: 'None'\n",
      "Input: 'l', Predicted next character: 'None'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "# Define the data\n",
    "word = \"hello\"\n",
    "\n",
    "# Define Hashing Function\n",
    "def hash_function(value, num_buckets):\n",
    "    hashed = int(hashlib.md5(value.encode()).hexdigest(), 16)\n",
    "    return hashed % num_buckets\n",
    "\n",
    "# Hashing Encoding (character-by-character for RNN compatibility)\n",
    "num_buckets = 5  # Define the number of buckets for hashing\n",
    "hash_vectors = []\n",
    "\n",
    "for ch in word:\n",
    "    hash_vector = np.zeros((num_buckets,), dtype=int)\n",
    "    bucket = hash_function(ch, num_buckets)\n",
    "    hash_vector[bucket] += 1\n",
    "    hash_vectors.append(hash_vector)\n",
    "\n",
    "print(\"Encoded Word using Hashing Encoding (character-by-character):\")\n",
    "for i, vec in enumerate(hash_vectors):\n",
    "    print(f\"Character '{word[i]}': {vec}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Define Hyperparameters and Model Parameters\n",
    "hidden_size = 1  # Single neuron\n",
    "input_size = num_buckets\n",
    "output_size = num_buckets\n",
    "learning_rate = 0.1\n",
    "\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((output_size, 1))\n",
    "\n",
    "# RNN function\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Predict next characters using RNN\n",
    "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "print(\"Predictions:\")\n",
    "\n",
    "for t in range(len(hash_vectors) - 1):  # Predict for all but the last character\n",
    "    x_t = hash_vectors[t].reshape(-1, 1)\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "\n",
    "    # Decode prediction (find the bucket with the highest value)\n",
    "    predicted_bucket = np.argmax(y_pred)\n",
    "    # Match the bucket back to a character\n",
    "    predicted_char = None\n",
    "    for ch in word:  # Iterate through word to find matching hash bucket\n",
    "        if hash_function(ch, num_buckets) == predicted_bucket:\n",
    "            predicted_char = ch\n",
    "            break\n",
    "\n",
    "    print(f\"Input: '{word[t]}', Predicted next character: '{predicted_char}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de602eb8-e939-4725-9c7e-edb065bc2d2f",
   "metadata": {},
   "source": [
    "# **Q3 - BackPropogation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725c1fb-3b30-4705-ac42-254650230271",
   "metadata": {},
   "source": [
    "## Next letter Prediction in Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b6745d24-b954-46d4-b3b6-2f414aadd82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Word using One-Hot Encoding: [0, 4, 2, 0, 3, 1, 5] \n",
      "\n",
      "Epoch 1/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00339093]]\n",
      "  Raw Prediction (y_pred): [[-2.66751680e-05  1.91017253e-05  1.38591605e-05  4.18017176e-05\n",
      "  -1.77171839e-05 -1.81790852e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16666188 0.16666951 0.16666864 0.1666733  0.16666338 0.1666633 ]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16666188  0.16666951  0.16666864  0.1666733  -0.83333662  0.1666633 ]]\n",
      "  Gradient Wy (dWy): [[ 0.00056514]\n",
      " [ 0.00056517]\n",
      " [ 0.00056516]\n",
      " [ 0.00056518]\n",
      " [-0.00282579]\n",
      " [ 0.00056514]]\n",
      "  Gradient Wh (dWh): [[1.97494238e-05]]\n",
      "  Gradient Wx (dWx): [[0.00582419 0.         0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00582419]]\n",
      "  Gradient by (dby): [[ 0.16666188  0.16666951  0.16666864  0.1666733  -0.83333662  0.1666633 ]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00828447]]\n",
      "  Raw Prediction (y_pred): [[-0.01660055 -0.01671315 -0.01670026 -0.01676899  0.08337461 -0.01662145]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16380709 0.16378865 0.16379076 0.1637795  0.18103034 0.16380367]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16380709  0.16378865 -0.83620924  0.1637795   0.18103034  0.16380367]]\n",
      "  Gradient Wy (dWy): [[-0.00135705]\n",
      " [-0.0013569 ]\n",
      " [ 0.00692755]\n",
      " [-0.00135683]\n",
      " [-0.00149974]\n",
      " [-0.00135703]]\n",
      "  Gradient Wh (dWh): [[2.92197544e-05]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.          0.         -0.00352705  0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00352705]]\n",
      "  Gradient by (dby): [[ 0.16380709  0.16378865 -0.83620924  0.1637795   0.18103034  0.16380367]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.01043699]]\n",
      "  Raw Prediction (y_pred): [[-0.03296562 -0.03310544  0.06691922 -0.03317477  0.06528065 -0.03299157]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16108499 0.16106246 0.17800594 0.1610513  0.1777145  0.16108081]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83891501  0.16106246  0.17800594  0.1610513   0.1777145   0.16108081]]\n",
      "  Gradient Wy (dWy): [[ 0.00875575]\n",
      " [-0.00168101]\n",
      " [-0.00185785]\n",
      " [-0.00168089]\n",
      " [-0.00185481]\n",
      " [-0.0016812 ]]\n",
      "  Gradient Wh (dWh): [[-8.70644937e-05]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.00834191 0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00834191]]\n",
      "  Gradient by (dby): [[-0.83891501  0.16106246  0.17800594  0.1610513   0.1777145   0.16108081]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00170279]]\n",
      "  Raw Prediction (y_pred): [[ 0.05082985 -0.04914205  0.04915947 -0.049129    0.04745133 -0.04916348]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17514548 0.15848264 0.17485317 0.15848471 0.17455475 0.15847924]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17514548  0.15848264  0.17485317 -0.84151529  0.17455475  0.15847924]]\n",
      "  Gradient Wy (dWy): [[ 0.00029824]\n",
      " [ 0.00026986]\n",
      " [ 0.00029774]\n",
      " [-0.00143292]\n",
      " [ 0.00029723]\n",
      " [ 0.00026986]]\n",
      "  Gradient Wh (dWh): [[-2.07154067e-05]]\n",
      "  Gradient Wx (dWx): [[-0.0121656  0.         0.         0.         0.         0.       ]]\n",
      "  Gradient bh (dbh): [[-0.0121656]]\n",
      "  Gradient by (dby): [[ 0.17514548  0.15848264  0.17485317 -0.84151529  0.17455475  0.15847924]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00969105]]\n",
      "  Raw Prediction (y_pred): [[ 0.03324581 -0.0649436   0.03170201  0.03512437  0.02995877 -0.06505252]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17211966 0.15602254 0.17185414 0.1724433  0.17155482 0.15600555]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17211966 -0.84397746  0.17185414  0.1724433   0.17155482  0.15600555]]\n",
      "  Gradient Wy (dWy): [[ 0.00166802]\n",
      " [-0.00817902]\n",
      " [ 0.00166545]\n",
      " [ 0.00167116]\n",
      " [ 0.00166255]\n",
      " [ 0.00151186]]\n",
      "  Gradient Wh (dWh): [[-5.07785767e-05]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.00523974  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00523974]]\n",
      "  Gradient by (dby): [[ 0.17211966 -0.84397746  0.17185414  0.1724433   0.17155482  0.15600555]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00190169]]\n",
      "  Raw Prediction (y_pred): [[ 0.01610124  0.01941011  0.01448906  0.01778066  0.01283909 -0.08061332]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16926366 0.16982466 0.168991   0.16954817 0.1687124  0.15366011]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16926366  0.16982466  0.168991    0.16954817  0.1687124  -0.84633989]]\n",
      "  Gradient Wy (dWy): [[ 0.00032189]\n",
      " [ 0.00032295]\n",
      " [ 0.00032137]\n",
      " [ 0.00032243]\n",
      " [ 0.00032084]\n",
      " [-0.00160948]]\n",
      "  Gradient Wh (dWh): [[1.13951138e-05]]\n",
      "  Gradient Wx (dWx): [[0.        0.0059921 0.        0.        0.        0.       ]]\n",
      "  Gradient bh (dbh): [[0.0059921]]\n",
      "  Gradient by (dby): [[ 0.16926366  0.16982466  0.168991    0.16954817  0.1687124  -0.84633989]]\n",
      "\n",
      "Epoch 1 Loss: [10.99963209]\n",
      "Epoch 2/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00410248]]\n",
      "  Raw Prediction (y_pred): [[-0.00084475  0.00244219 -0.00240285  0.00085333 -0.00404285  0.00400968]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16652489 0.16707315 0.16626563 0.16680791 0.16599318 0.16733524]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16652489  0.16707315  0.16626563  0.16680791 -0.83400682  0.16733524]]\n",
      "  Gradient Wy (dWy): [[ 0.00068317]\n",
      " [ 0.00068541]\n",
      " [ 0.0006821 ]\n",
      " [ 0.00068433]\n",
      " [-0.0034215 ]\n",
      " [ 0.00068649]]\n",
      "  Gradient Wh (dWh): [[2.23103029e-05]]\n",
      "  Gradient Wx (dWx): [[0.00543824 0.         0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00543824]]\n",
      "  Gradient by (dby): [[ 0.16652489  0.16707315  0.16626563  0.16680791 -0.83400682  0.16733524]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00781289]]\n",
      "  Raw Prediction (y_pred): [[-0.01739076 -0.0143437  -0.01906814 -0.01597609  0.07941277 -0.01266217]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16368822 0.16418775 0.16341388 0.16391995 0.18032613 0.16446407]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16368822  0.16418775 -0.83658612  0.16391995  0.18032613  0.16446407]]\n",
      "  Gradient Wy (dWy): [[-0.00127888]\n",
      " [-0.00128278]\n",
      " [ 0.00653616]\n",
      " [-0.00128069]\n",
      " [-0.00140887]\n",
      " [-0.00128494]]\n",
      "  Gradient Wh (dWh): [[2.11870926e-05]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.          0.         -0.00271181  0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00271181]]\n",
      "  Gradient by (dby): [[ 0.16368822  0.16418775 -0.83658612  0.16391995  0.18032613  0.16446407]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.0112348]]\n",
      "  Raw Prediction (y_pred): [[-0.03373036 -0.0307864   0.06458677 -0.03241213  0.06139394 -0.02909222]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16097812 0.16145273 0.17760919 0.16119046 0.17704302 0.16172649]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83902188  0.16145273  0.17760919  0.16119046  0.17704302  0.16172649]]\n",
      "  Gradient Wy (dWy): [[ 0.00942624]\n",
      " [-0.00181389]\n",
      " [-0.0019954 ]\n",
      " [-0.00181094]\n",
      " [-0.00198904]\n",
      " [-0.00181696]]\n",
      "  Gradient Wh (dWh): [[-0.00010544]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.00938475 0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00938475]]\n",
      "  Gradient by (dby): [[-0.83902188  0.16145273  0.17760919  0.16119046  0.17704302  0.16172649]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00230239]]\n",
      "  Raw Prediction (y_pred): [[ 0.0500501  -0.04684057  0.04686115 -0.04836048  0.04363119 -0.0453331 ]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17502792 0.15886501 0.17447065 0.15862373 0.17390803 0.15910467]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17502792  0.15886501  0.17447065 -0.84137627  0.17390803  0.15910467]]\n",
      "  Gradient Wy (dWy): [[ 0.00040298]\n",
      " [ 0.00036577]\n",
      " [ 0.0004017 ]\n",
      " [-0.00193718]\n",
      " [ 0.0004004 ]\n",
      " [ 0.00036632]]\n",
      "  Gradient Wh (dWh): [[-2.84797265e-05]]\n",
      "  Gradient Wx (dWx): [[-0.01236963  0.          0.          0.          0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01236963]]\n",
      "  Gradient by (dby): [[ 0.17502792  0.15886501  0.17447065 -0.84137627  0.17390803  0.15910467]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01016802]]\n",
      "  Raw Prediction (y_pred): [[ 0.03247001 -0.06267332  0.02943549  0.03587948  0.02620731 -0.0612824 ]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17200179 0.15639135 0.17148064 0.17258922 0.17092796 0.15660903]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17200179 -0.84360865  0.17148064  0.17258922  0.17092796  0.15660903]]\n",
      "  Gradient Wy (dWy): [[ 0.00174892]\n",
      " [-0.00857783]\n",
      " [ 0.00174362]\n",
      " [ 0.00175489]\n",
      " [ 0.001738  ]\n",
      " [ 0.0015924 ]]\n",
      "  Gradient Wh (dWh): [[-6.34983327e-05]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.00624491  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00624491]]\n",
      "  Gradient by (dby): [[ 0.17200179 -0.84360865  0.17148064  0.17258922  0.17092796  0.15660903]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00135567]]\n",
      "  Raw Prediction (y_pred): [[ 0.01535609  0.0216284   0.0122631   0.01850617  0.00915123 -0.07690011]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16914622 0.17021049 0.16862386 0.16967988 0.16809994 0.15423961]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16914622  0.17021049  0.16862386  0.16967988  0.16809994 -0.84576039]]\n",
      "  Gradient Wy (dWy): [[ 0.00022931]\n",
      " [ 0.00023075]\n",
      " [ 0.0002286 ]\n",
      " [ 0.00023003]\n",
      " [ 0.00022789]\n",
      " [-0.00114657]]\n",
      "  Gradient Wh (dWh): [[7.84592945e-06]]\n",
      "  Gradient Wx (dWx): [[0.         0.00578749 0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00578749]]\n",
      "  Gradient by (dby): [[ 0.16914622  0.17021049  0.16862386  0.16967988  0.16809994 -0.84576039]]\n",
      "\n",
      "Epoch 2 Loss: [10.99962661]\n",
      "Epoch 3/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.0048672]]\n",
      "  Raw Prediction (y_pred): [[-0.00159373  0.00463428 -0.00459041  0.00158294 -0.00767421  0.00765863]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16639848 0.16743805 0.16590058 0.16692791 0.16538977 0.1679452 ]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16639848  0.16743805  0.16590058  0.16692791 -0.83461023  0.1679452 ]]\n",
      "  Gradient Wy (dWy): [[ 0.00080989]\n",
      " [ 0.00081495]\n",
      " [ 0.00080747]\n",
      " [ 0.00081247]\n",
      " [-0.00406221]\n",
      " [ 0.00081742]]\n",
      "  Gradient Wh (dWh): [[2.43258474e-05]]\n",
      "  Gradient Wx (dWx): [[0.00499792 0.         0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00499792]]\n",
      "  Gradient by (dby): [[ 0.16639848  0.16743805  0.16590058  0.16692791 -0.83461023  0.1679452 ]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00742298]]\n",
      "  Raw Prediction (y_pred): [[-0.01810991 -0.01220329 -0.02121103 -0.01526601  0.07583775 -0.0090742 ]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16357899 0.16454804 0.16307249 0.16404485 0.17969188 0.16506374]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16357899  0.16454804 -0.83692751  0.16404485  0.17969188  0.16506374]]\n",
      "  Gradient Wy (dWy): [[-0.00121424]\n",
      " [-0.00122144]\n",
      " [ 0.00621249]\n",
      " [-0.0012177 ]\n",
      " [-0.00133385]\n",
      " [-0.00122526]]\n",
      "  Gradient Wh (dWh): [[1.42982126e-05]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.          0.         -0.00192621  0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00192621]]\n",
      "  Gradient by (dby): [[ 0.16357899  0.16454804 -0.83692751  0.16404485  0.17969188  0.16506374]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.01213464]]\n",
      "  Raw Prediction (y_pred): [[-0.03442172 -0.02869537  0.06247769 -0.03173168  0.05788571 -0.02555825]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16088058 0.16180448 0.1772501  0.16131393 0.17643804 0.16231287]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83911942  0.16180448  0.1772501   0.16131393  0.17643804  0.16231287]]\n",
      "  Gradient Wy (dWy): [[ 0.01018241]\n",
      " [-0.00196344]\n",
      " [-0.00215087]\n",
      " [-0.00195749]\n",
      " [-0.00214101]\n",
      " [-0.00196961]]\n",
      "  Gradient Wh (dWh): [[-0.00012772]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.01052514 0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.01052514]]\n",
      "  Gradient by (dby): [[-0.83911942  0.16180448  0.1772501   0.16131393  0.17643804  0.16231287]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00295875]]\n",
      "  Raw Prediction (y_pred): [[ 0.04933669 -0.04475873  0.04478099 -0.04766937  0.04018443 -0.04186336]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17491919 0.15921073 0.17412412 0.158748   0.17332559 0.15967237]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17491919  0.15921073  0.17412412 -0.841252    0.17332559  0.15967237]]\n",
      "  Gradient Wy (dWy): [[ 0.00051754]\n",
      " [ 0.00047106]\n",
      " [ 0.00051519]\n",
      " [-0.00248905]\n",
      " [ 0.00051283]\n",
      " [ 0.00047243]]\n",
      "  Gradient Wh (dWh): [[-3.73394662e-05]]\n",
      "  Gradient Wx (dWx): [[-0.01262002  0.          0.          0.          0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01262002]]\n",
      "  Gradient by (dby): [[ 0.17491919  0.15921073  0.17412412 -0.841252    0.17332559  0.15967237]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.0107432]]\n",
      "  Raw Prediction (y_pred): [[ 0.03175865 -0.06061869  0.02738397  0.03655963  0.02282302 -0.05786795]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17189263 0.156725   0.1711423  0.17271987 0.1703635  0.15715671]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17189263 -0.843275    0.1711423   0.17271987  0.1703635   0.15715671]]\n",
      "  Gradient Wy (dWy): [[ 0.00184668]\n",
      " [-0.00905947]\n",
      " [ 0.00183862]\n",
      " [ 0.00185556]\n",
      " [ 0.00183025]\n",
      " [ 0.00168837]]\n",
      "  Gradient Wh (dWh): [[-7.82447605e-05]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.00728319  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00728319]]\n",
      "  Gradient by (dby): [[ 0.17189263 -0.843275    0.1711423   0.17271987  0.1703635   0.15715671]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00083134]]\n",
      "  Raw Prediction (y_pred): [[ 0.01467869  0.02363157  0.0102498   0.01915625  0.00582306 -0.07353638]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16903859 0.17055877 0.16829159 0.16979717 0.16754825 0.15476564]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16903859  0.17055877  0.16829159  0.16979717  0.16754825 -0.84523436]]\n",
      "  Gradient Wy (dWy): [[ 0.00014053]\n",
      " [ 0.00014179]\n",
      " [ 0.00013991]\n",
      " [ 0.00014116]\n",
      " [ 0.00013929]\n",
      " [-0.00070267]]\n",
      "  Gradient Wh (dWh): [[4.70123495e-06]]\n",
      "  Gradient Wx (dWx): [[0.         0.00565503 0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00565503]]\n",
      "  Gradient by (dby): [[ 0.16903859  0.17055877  0.16829159  0.16979717  0.16754825 -0.84523436]]\n",
      "\n",
      "Epoch 3 Loss: [10.99964691]\n",
      "Epoch 4/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00569451]]\n",
      "  Raw Prediction (y_pred): [[-0.00227985  0.00661828 -0.00657062  0.00223994 -0.01095067  0.01096339]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16628188 0.16776808 0.16556993 0.16703514 0.16484632 0.16849864]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16628188  0.16776808  0.16556993  0.16703514 -0.83515368  0.16849864]]\n",
      "  Gradient Wy (dWy): [[ 0.00094689]\n",
      " [ 0.00095536]\n",
      " [ 0.00094284]\n",
      " [ 0.00095118]\n",
      " [-0.0047558 ]\n",
      " [ 0.00095952]]\n",
      "  Gradient Wh (dWh): [[2.56145629e-05]]\n",
      "  Gradient Wx (dWx): [[0.00449811 0.         0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00449811]]\n",
      "  Gradient by (dby): [[ 0.16628188  0.16776808  0.16556993  0.16703514 -0.83515368  0.16849864]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00711181]]\n",
      "  Raw Prediction (y_pred): [[-0.01876341 -0.01027003 -0.02314999 -0.01462989  0.07261106 -0.00582332]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16347888 0.16487328 0.16276334 0.16415602 0.17912041 0.16560806]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16347888  0.16487328 -0.83723666  0.16415602  0.17912041  0.16560806]]\n",
      "  Gradient Wy (dWy): [[-0.00116263]\n",
      " [-0.00117255]\n",
      " [ 0.00595427]\n",
      " [-0.00116745]\n",
      " [-0.00127387]\n",
      " [-0.00117777]]\n",
      "  Gradient Wh (dWh): [[8.25977522e-06]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.          0.         -0.00116142  0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00116142]]\n",
      "  Gradient by (dby): [[ 0.16347888  0.16487328 -0.83723666  0.16415602  0.17912041  0.16560806]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.01314725]]\n",
      "  Raw Prediction (y_pred): [[-0.0350444  -0.02681119  0.06057122 -0.03112516  0.05471792 -0.02235565]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16079202 0.16212132 0.17692526 0.16142344 0.17589269 0.16284527]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83920798  0.16212132  0.17692526  0.16142344  0.17589269  0.16284527]]\n",
      "  Gradient Wy (dWy): [[ 0.01103328]\n",
      " [-0.00213145]\n",
      " [-0.00232608]\n",
      " [-0.00212227]\n",
      " [-0.0023125 ]\n",
      " [-0.00214097]]\n",
      "  Gradient Wh (dWh): [[-0.0001548]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.01177404 0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.01177404]]\n",
      "  Gradient by (dby): [[-0.83920798  0.16212132  0.17692526  0.16142344  0.17589269  0.16284527]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00368028]]\n",
      "  Raw Prediction (y_pred): [[ 0.04868353 -0.04287477  0.04289822 -0.04704692  0.03707429 -0.03872112]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17481868 0.15952346 0.17381022 0.15885929 0.1728009  0.16018745]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17481868  0.15952346  0.17381022 -0.84114071  0.1728009   0.16018745]]\n",
      "  Gradient Wy (dWy): [[ 0.00064338]\n",
      " [ 0.00058709]\n",
      " [ 0.00063967]\n",
      " [-0.00309564]\n",
      " [ 0.00063596]\n",
      " [ 0.00058954]]\n",
      "  Gradient Wh (dWh): [[-4.75566204e-05]]\n",
      "  Gradient Wx (dWx): [[-0.012922  0.        0.        0.        0.        0.      ]]\n",
      "  Gradient bh (dbh): [[-0.012922]]\n",
      "  Gradient by (dby): [[ 0.17481868  0.15952346  0.17381022 -0.84114071  0.1728009   0.16018745]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01141838]]\n",
      "  Raw Prediction (y_pred): [[ 0.03110557 -0.05875818  0.02552685  0.0371734   0.01976987 -0.05477646]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17179151 0.15702701 0.1708358  0.17283708 0.16985512 0.15765349]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17179151 -0.84297299  0.1708358   0.17283708  0.16985512  0.15765349]]\n",
      "  Gradient Wy (dWy): [[ 0.00196158]\n",
      " [-0.00962539]\n",
      " [ 0.00195067]\n",
      " [ 0.00197352]\n",
      " [ 0.00193947]\n",
      " [ 0.00180015]]\n",
      "  Gradient Wh (dWh): [[-9.55018777e-05]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.00836387  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00836387]]\n",
      "  Gradient by (dby): [[ 0.17179151 -0.84297299  0.1708358   0.17283708  0.16985512  0.15765349]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00032088]]\n",
      "  Raw Prediction (y_pred): [[ 0.01406383  0.02544026  0.00842902  0.01973888  0.00281885 -0.07048969]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.1689402  0.17087312 0.16799094 0.16990168 0.16705112 0.15524295]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.1689402   0.17087312  0.16799094  0.16990168  0.16705112 -0.84475705]]\n",
      "  Gradient Wy (dWy): [[ 5.42087093e-05]\n",
      " [ 5.48289322e-05]\n",
      " [ 5.39041128e-05]\n",
      " [ 5.45172212e-05]\n",
      " [ 5.36025479e-05]\n",
      " [-2.71061523e-04]]\n",
      "  Gradient Wh (dWh): [[1.794737e-06]]\n",
      "  Gradient Wx (dWx): [[0.         0.00559326 0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00559326]]\n",
      "  Gradient by (dby): [[ 0.1689402   0.17087312  0.16799094  0.16990168  0.16705112 -0.84475705]]\n",
      "\n",
      "Epoch 4 Loss: [10.99968108]\n",
      "Epoch 5/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00659506]]\n",
      "  Raw Prediction (y_pred): [[-0.00290918  0.00841502 -0.00836331  0.00283267 -0.01390705  0.01395557]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.1661743  0.16806678 0.16527043 0.16713119 0.16435675 0.16900055]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.1661743   0.16806678  0.16527043  0.16713119 -0.83564325  0.16900055]]\n",
      "  Gradient Wy (dWy): [[ 0.00109593]\n",
      " [ 0.00110841]\n",
      " [ 0.00108997]\n",
      " [ 0.00110224]\n",
      " [-0.00551112]\n",
      " [ 0.00111457]]\n",
      "  Gradient Wh (dWh): [[2.5937815e-05]]\n",
      "  Gradient Wx (dWx): [[0.00393292 0.         0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00393292]]\n",
      "  Gradient by (dby): [[ 0.1661743   0.16806678  0.16527043  0.16713119 -0.83564325  0.16900055]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00687716]]\n",
      "  Raw Prediction (y_pred): [[-0.01935625 -0.00852418 -0.02490412 -0.01405996  0.06969812 -0.00287832]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16338739 0.16516684 0.16248345 0.16425503 0.17860531 0.16610198]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16338739  0.16516684 -0.83751655  0.16425503  0.17860531  0.16610198]]\n",
      "  Gradient Wy (dWy): [[-0.00112364]\n",
      " [-0.00113588]\n",
      " [ 0.00575973]\n",
      " [-0.00112961]\n",
      " [-0.0012283 ]\n",
      " [-0.00114231]]\n",
      "  Gradient Wh (dWh): [[2.81088398e-06]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.          0.         -0.00040873  0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00040873]]\n",
      "  Gradient by (dby): [[ 0.16338739  0.16516684 -0.83751655  0.16425503  0.17860531  0.16610198]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.01428423]]\n",
      "  Raw Prediction (y_pred): [[-0.03560254 -0.02511495  0.05884859 -0.03058538  0.05185629 -0.01945337]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16071213 0.16240648 0.17663154 0.16152048 0.17540079 0.16332857]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83928787  0.16240648  0.17663154  0.16152048  0.17540079  0.16332857]]\n",
      "  Gradient Wy (dWy): [[ 0.01198858]\n",
      " [-0.00231985]\n",
      " [-0.00252305]\n",
      " [-0.0023072 ]\n",
      " [-0.00250547]\n",
      " [-0.00233302]]\n",
      "  Gradient Wh (dWh): [[-0.00018775]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.01314366 0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.01314366]]\n",
      "  Gradient by (dby): [[-0.83928787  0.16240648  0.17663154  0.16152048  0.17540079  0.16332857]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00447661]]\n",
      "  Raw Prediction (y_pred): [[ 0.04808475 -0.0411689   0.04119393 -0.0464853   0.03426782 -0.0358762 ]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17472573 0.15980652 0.17352586 0.15895918 0.17232815 0.16065457]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17472573  0.15980652  0.17352586 -0.84104082  0.17232815  0.16065457]]\n",
      "  Gradient Wy (dWy): [[ 0.00078218]\n",
      " [ 0.00071539]\n",
      " [ 0.00077681]\n",
      " [-0.00376501]\n",
      " [ 0.00077145]\n",
      " [ 0.00071919]]\n",
      "  Gradient Wh (dWh): [[-5.94564213e-05]]\n",
      "  Gradient Wx (dWx): [[-0.01328158  0.          0.          0.          0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01328158]]\n",
      "  Gradient by (dby): [[ 0.17472573  0.15980652  0.17352586 -0.84104082  0.17232815  0.16065457]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01219647]]\n",
      "  Raw Prediction (y_pred): [[ 0.03050481 -0.05707224  0.02384537  0.03772847  0.01701563 -0.05197819]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17169772 0.15730057 0.17055811 0.17294249 0.16939721 0.15810391]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17169772 -0.84269943  0.17055811  0.17294249  0.16939721  0.15810391]]\n",
      "  Gradient Wy (dWy): [[ 0.00209411]\n",
      " [-0.01027796]\n",
      " [ 0.00208021]\n",
      " [ 0.00210929]\n",
      " [ 0.00206605]\n",
      " [ 0.00192831]]\n",
      "  Gradient Wh (dWh): [[-0.00011582]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.00949626  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00949626]]\n",
      "  Gradient by (dby): [[ 0.17169772 -0.84269943  0.17055811  0.17294249  0.16939721  0.15810391]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.0001832]]\n",
      "  Raw Prediction (y_pred): [[ 0.0135067   0.02707302  0.00678258  0.02026106  0.00010647 -0.06773048]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16885051 0.1711568  0.16771894 0.16999484 0.16660296 0.15567594]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16885051  0.1711568   0.16771894  0.16999484  0.16660296 -0.84432406]]\n",
      "  Gradient Wy (dWy): [[-3.09335538e-05]\n",
      " [-3.13560680e-05]\n",
      " [-3.07262507e-05]\n",
      " [-3.11431974e-05]\n",
      " [-3.05218020e-05]\n",
      " [ 1.54680872e-04]]\n",
      "  Gradient Wh (dWh): [[-1.02623214e-06]]\n",
      "  Gradient Wx (dWx): [[0.         0.00560168 0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00560168]]\n",
      "  Gradient by (dby): [[ 0.16885051  0.1711568   0.16771894  0.16999484  0.16660296 -0.84432406]]\n",
      "\n",
      "Epoch 5 Loss: [10.999719]\n",
      "Epoch 6/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00758071]]\n",
      "  Raw Prediction (y_pred): [[-0.00348759  0.01004342 -0.0099865   0.00336858 -0.01657455  0.01666389]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16607491 0.16833735 0.16499911 0.16721746 0.16391566 0.16945552]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16607491  0.16833735  0.16499911  0.16721746 -0.83608434  0.16945552]]\n",
      "  Gradient Wy (dWy): [[ 0.00125897]\n",
      " [ 0.00127612]\n",
      " [ 0.00125081]\n",
      " [ 0.00126763]\n",
      " [-0.00633811]\n",
      " [ 0.00128459]]\n",
      "  Gradient Wh (dWh): [[2.49825309e-05]]\n",
      "  Gradient Wx (dWx): [[0.00329554 0.         0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00329554]]\n",
      "  Gradient by (dby): [[ 0.16607491  0.16833735  0.16499911  0.16721746 -0.83608434  0.16945552]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00671746]]\n",
      "  Raw Prediction (y_pred): [[-0.01989307 -0.00694798 -0.02649076 -0.01354938  0.06706779 -0.00021075]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16330401 0.16543174 0.16223013 0.16434325 0.17814083 0.16655005]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16330401  0.16543174 -0.83776987  0.16434325  0.17814083  0.16655005]]\n",
      "  Gradient Wy (dWy): [[-0.00109699]\n",
      " [-0.00111128]\n",
      " [ 0.00562768]\n",
      " [-0.00110397]\n",
      " [-0.00119665]\n",
      " [-0.00111879]]\n",
      "  Gradient Wh (dWh): [[-2.28743569e-06]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00034052 0.        ]]\n",
      "  Gradient bh (dbh): [[0.00034052]]\n",
      "  Gradient by (dby): [[ 0.16330401  0.16543174 -0.83776987  0.16434325  0.17814083  0.16655005]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.01555825]]\n",
      "  Raw Prediction (y_pred): [[-0.03609967 -0.02358978  0.05729289 -0.03010616  0.04926983 -0.01682305]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16064062 0.16266284 0.17636615 0.16160631 0.17495682 0.16376726]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83935938  0.16266284  0.17636615  0.16160631  0.17495682  0.16376726]]\n",
      "  Gradient Wy (dWy): [[ 0.01305896]\n",
      " [-0.00253075]\n",
      " [-0.00274395]\n",
      " [-0.00251431]\n",
      " [-0.00272202]\n",
      " [-0.00254793]]\n",
      "  Gradient Wh (dWh): [[-0.00022789]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.01464752 0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.01464752]]\n",
      "  Gradient by (dby): [[-0.83935938  0.16266284  0.17636615  0.16160631  0.17495682  0.16376726]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00535864]]\n",
      "  Raw Prediction (y_pred): [[ 0.04753464 -0.03962314  0.03965094 -0.04597749  0.03173554 -0.03330123]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17463964 0.1600629  0.17326825 0.15904903 0.17190217 0.16107801]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17463964  0.1600629   0.17326825 -0.84095097  0.17190217  0.16107801]]\n",
      "  Gradient Wy (dWy): [[ 0.00093583]\n",
      " [ 0.00085772]\n",
      " [ 0.00092848]\n",
      " [-0.00450636]\n",
      " [ 0.00092116]\n",
      " [ 0.00086316]]\n",
      "  Gradient Wh (dWh): [[-7.34431095e-05]]\n",
      "  Gradient Wx (dWx): [[-0.01370554  0.          0.          0.          0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01370554]]\n",
      "  Gradient by (dby): [[ 0.17463964  0.1600629   0.17326825 -0.84095097  0.17190217  0.16107801]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01308144]]\n",
      "  Raw Prediction (y_pred): [[ 0.02995058 -0.0555431   0.0223225   0.03823176  0.01453143 -0.04944615]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17161054 0.15754859 0.17030646 0.17303758 0.16898475 0.15851209]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17161054 -0.84245141  0.17030646  0.17303758  0.16898475  0.15851209]]\n",
      "  Gradient Wy (dWy): [[ 0.00224491]\n",
      " [-0.01102048]\n",
      " [ 0.00222785]\n",
      " [ 0.00226358]\n",
      " [ 0.00221056]\n",
      " [ 0.00207357]]\n",
      "  Gradient Wh (dWh): [[-0.00013984]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.01068979  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01068979]]\n",
      "  Gradient by (dby): [[ 0.17161054 -0.84245141  0.17030646  0.17303758  0.16898475  0.15851209]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00068818]]\n",
      "  Raw Prediction (y_pred): [[ 0.01300291  0.02854654  0.00529399  0.02072893 -0.002343   -0.06523185]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16876896 0.17141273 0.16747293 0.17007792 0.16619882 0.15606864]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16876896  0.17141273  0.16747293  0.17007792  0.16619882 -0.84393136]]\n",
      "  Gradient Wy (dWy): [[-0.00011614]\n",
      " [-0.00011796]\n",
      " [-0.00011525]\n",
      " [-0.00011704]\n",
      " [-0.00011437]\n",
      " [ 0.00058077]]\n",
      "  Gradient Wh (dWh): [[-3.90934973e-06]]\n",
      "  Gradient Wx (dWx): [[0.         0.00568073 0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00568073]]\n",
      "  Gradient by (dby): [[ 0.16876896  0.17141273  0.16747293  0.17007792  0.16619882 -0.84393136]]\n",
      "\n",
      "Epoch 6 Loss: [10.99975179]\n",
      "Epoch 7/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00866474]]\n",
      "  Raw Prediction (y_pred): [[-0.00402082  0.01152079 -0.01145663  0.00385445 -0.01898108  0.01911444]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.1659829  0.16858269 0.16475326 0.16729522 0.16351823 0.16986772]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.1659829   0.16858269  0.16475326  0.16729522 -0.83648177  0.16986772]]\n",
      "  Gradient Wy (dWy): [[ 0.0014382 ]\n",
      " [ 0.00146072]\n",
      " [ 0.00142754]\n",
      " [ 0.00144957]\n",
      " [-0.00724789]\n",
      " [ 0.00147186]]\n",
      "  Gradient Wh (dWh): [[2.23393863e-05]]\n",
      "  Gradient Wx (dWx): [[0.0025782 0.        0.        0.        0.        0.       ]]\n",
      "  Gradient bh (dbh): [[0.0025782]]\n",
      "  Gradient by (dby): [[ 0.1659829   0.16858269  0.16475326  0.16729522 -0.83648177  0.16986772]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00663186]]\n",
      "  Raw Prediction (y_pred): [[-0.02037803 -0.00552546 -0.02792558 -0.01309215  0.06469205  0.00220532]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16322825 0.1656707  0.16200092 0.16442186 0.17772184 0.16695643]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16322825  0.1656707  -0.83799908  0.16442186  0.17772184  0.16695643]]\n",
      "  Gradient Wy (dWy): [[-0.00108251]\n",
      " [-0.0010987 ]\n",
      " [ 0.00555749]\n",
      " [-0.00109042]\n",
      " [-0.00117863]\n",
      " [-0.00110723]]\n",
      "  Gradient Wh (dWh): [[-7.26223128e-06]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00109505 0.        ]]\n",
      "  Gradient bh (dbh): [[0.00109505]]\n",
      "  Gradient by (dby): [[ 0.16322825  0.1656707  -0.83799908  0.16442186  0.17772184  0.16695643]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.01698318]]\n",
      "  Raw Prediction (y_pred): [[-0.03653871 -0.02222074  0.05588892 -0.02968223  0.04693049 -0.0144388 ]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16057725 0.16289292 0.17612654 0.16168202 0.17455577 0.16416549]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83942275  0.16289292  0.17612654  0.16168202  0.17455577  0.16416549]]\n",
      "  Gradient Wy (dWy): [[ 0.01425607]\n",
      " [-0.00276644]\n",
      " [-0.00299119]\n",
      " [-0.00274587]\n",
      " [-0.00296451]\n",
      " [-0.00278805]]\n",
      "  Gradient Wh (dWh): [[-0.00027684]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.01630061 0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.01630061]]\n",
      "  Gradient by (dby): [[-0.83942275  0.16289292  0.17612654  0.16168202  0.17455577  0.16416549]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00633872]]\n",
      "  Raw Prediction (y_pred): [[ 0.04702759 -0.03822105  0.03825359 -0.0455171   0.02945108 -0.03097133]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.1745597  0.16029536 0.17303481 0.1591301  0.17151835 0.16146168]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.1745597   0.16029536  0.17303481 -0.8408699   0.17151835  0.16146168]]\n",
      "  Gradient Wy (dWy): [[ 0.00110649]\n",
      " [ 0.00101607]\n",
      " [ 0.00109682]\n",
      " [-0.00533004]\n",
      " [ 0.00108721]\n",
      " [ 0.00102346]]\n",
      "  Gradient Wh (dWh): [[-9.00201456e-05]]\n",
      "  Gradient Wx (dWx): [[-0.01420163  0.          0.          0.          0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01420163]]\n",
      "  Gradient by (dby): [[ 0.1745597   0.16029536  0.17303481 -0.8408699   0.17151835  0.16146168]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01407828]]\n",
      "  Raw Prediction (y_pred): [[ 0.02943726 -0.05415456  0.02094273  0.03868956  0.01229148 -0.04715585]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17152922 0.15777372 0.17007834 0.17312363 0.16861329 0.1588818 ]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17152922 -0.84222628  0.17007834  0.17312363  0.16861329  0.1588818 ]]\n",
      "  Gradient Wy (dWy): [[ 0.00241484]\n",
      " [-0.0118571 ]\n",
      " [ 0.00239441]\n",
      " [ 0.00243728]\n",
      " [ 0.00237379]\n",
      " [ 0.00223678]]\n",
      "  Gradient Wh (dWh): [[-0.00016829]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.01195408  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01195408]]\n",
      "  Gradient by (dby): [[ 0.17152922 -0.84222628  0.17007834  0.17312363  0.16861329  0.1588818 ]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00120126]]\n",
      "  Raw Prediction (y_pred): [[ 0.01254851  0.02987579  0.00394839  0.0211479  -0.00455559 -0.06296932]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16869505 0.17164355 0.16725048 0.17015199 0.16583421 0.15642472]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16869505  0.17164355  0.16725048  0.17015199  0.16583421 -0.84357528]]\n",
      "  Gradient Wy (dWy): [[-0.00020265]\n",
      " [-0.00020619]\n",
      " [-0.00020091]\n",
      " [-0.0002044 ]\n",
      " [-0.00019921]\n",
      " [ 0.00101336]]\n",
      "  Gradient Wh (dWh): [[-7.00554302e-06]]\n",
      "  Gradient Wx (dWx): [[0.         0.00583181 0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00583181]]\n",
      "  Gradient by (dby): [[ 0.16869505  0.17164355  0.16725048  0.17015199  0.16583421 -0.84357528]]\n",
      "\n",
      "Epoch 7 Loss: [10.99977132]\n",
      "Epoch 8/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00986198]]\n",
      "  Raw Prediction (y_pred): [[-0.00451453  0.01286294 -0.01278867  0.00429644 -0.02115156  0.02133083]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.1658974  0.16880547 0.1645304  0.16736557 0.16316019 0.17024097]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.1658974   0.16880547  0.1645304   0.16736557 -0.83683981  0.17024097]]\n",
      "  Gradient Wy (dWy): [[ 0.00163608]\n",
      " [ 0.00166476]\n",
      " [ 0.0016226 ]\n",
      " [ 0.00165056]\n",
      " [-0.0082529 ]\n",
      " [ 0.00167891]]\n",
      "  Gradient Wh (dWh): [[1.74752084e-05]]\n",
      "  Gradient Wx (dWx): [[0.00177198 0.         0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00177198]]\n",
      "  Gradient by (dby): [[ 0.1658974   0.16880547  0.1645304   0.16736557 -0.83683981  0.17024097]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00662025]]\n",
      "  Raw Prediction (y_pred): [[-0.0208149  -0.00424226 -0.02922278 -0.012683    0.06254562  0.00439351]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16315967 0.16588619 0.16179359 0.16449188 0.17734373 0.16732495]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16315967  0.16588619 -0.83820641  0.16449188  0.17734373  0.16732495]]\n",
      "  Gradient Wy (dWy): [[-0.00108016]\n",
      " [-0.00109821]\n",
      " [ 0.00554913]\n",
      " [-0.00108898]\n",
      " [-0.00117406]\n",
      " [-0.00110773]]\n",
      "  Gradient Wh (dWh): [[-1.23384498e-05]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00186374 0.        ]]\n",
      "  Gradient bh (dbh): [[0.00186374]]\n",
      "  Gradient by (dby): [[ 0.16315967  0.16588619 -0.83820641  0.16449188  0.17734373  0.16732495]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.01857429]]\n",
      "  Raw Prediction (y_pred): [[-0.03692182 -0.02099466  0.05462306 -0.02930918  0.04481273 -0.0122769 ]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16052186 0.16309898 0.17591044 0.16174852 0.17419314 0.16452706]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83947814  0.16309898  0.17591044  0.16174852  0.17419314  0.16452706]]\n",
      "  Gradient Wy (dWy): [[ 0.01559271]\n",
      " [-0.00302945]\n",
      " [-0.00326741]\n",
      " [-0.00300436]\n",
      " [-0.00323551]\n",
      " [-0.00305597]]\n",
      "  Gradient Wh (dWh): [[-0.00033656]]\n",
      "  Gradient Wx (dWx): [[0.        0.        0.0181195 0.        0.        0.       ]]\n",
      "  Gradient bh (dbh): [[0.0181195]]\n",
      "  Gradient by (dby): [[-0.83947814  0.16309898  0.17591044  0.16174852  0.17419314  0.16452706]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00743075]]\n",
      "  Raw Prediction (y_pred): [[ 0.0465581  -0.03694759  0.03698762 -0.04509833  0.0273909  -0.02886398]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17448515 0.16050642 0.1728232  0.15920349 0.1711726  0.16180915]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17448515  0.16050642  0.1728232  -0.84079651  0.1711726   0.16180915]]\n",
      "  Gradient Wy (dWy): [[ 0.00129655]\n",
      " [ 0.00119268]\n",
      " [ 0.00128421]\n",
      " [-0.00624774]\n",
      " [ 0.00127194]\n",
      " [ 0.00120236]]\n",
      "  Gradient Wh (dWh): [[-0.00010982]]\n",
      "  Gradient Wx (dWx): [[-0.0147786  0.         0.         0.         0.         0.       ]]\n",
      "  Gradient bh (dbh): [[-0.0147786]]\n",
      "  Gradient by (dby): [[ 0.17448515  0.16050642  0.1728232  -0.84079651  0.1711726   0.16180915]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01519304]]\n",
      "  Raw Prediction (y_pred): [[ 0.02895931 -0.05289181  0.0196919   0.03910761  0.01027275 -0.04508514]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17145301 0.15797837 0.16987143 0.17320183 0.16827889 0.15921648]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17145301 -0.84202163  0.16987143  0.17320183  0.16827889  0.15921648]]\n",
      "  Gradient Wy (dWy): [[ 0.00260489]\n",
      " [-0.01279287]\n",
      " [ 0.00258086]\n",
      " [ 0.00263146]\n",
      " [ 0.00255667]\n",
      " [ 0.00241898]]\n",
      "  Gradient Wh (dWh): [[-0.00020205]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.01329901  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01329901]]\n",
      "  Gradient by (dby): [[ 0.17145301 -0.84202163  0.16987143  0.17320183  0.16827889  0.15921648]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00172971]]\n",
      "  Raw Prediction (y_pred): [[ 0.01213998  0.0310742   0.00273233  0.02152268 -0.00655482 -0.06092058]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16862834 0.1718516  0.16704938 0.17021797 0.16550515 0.15674755]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16862834  0.1718516   0.16704938  0.17021797  0.16550515 -0.84325245]]\n",
      "  Gradient Wy (dWy): [[-0.00029168]\n",
      " [-0.00029725]\n",
      " [-0.00028895]\n",
      " [-0.00029443]\n",
      " [-0.00028628]\n",
      " [ 0.00145858]]\n",
      "  Gradient Wh (dWh): [[-1.04773263e-05]]\n",
      "  Gradient Wx (dWx): [[0.         0.00605726 0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00605726]]\n",
      "  Gradient by (dby): [[ 0.16862834  0.1718516   0.16704938  0.17021797  0.16550515 -0.84325245]]\n",
      "\n",
      "Epoch 8 Loss: [10.9997698]\n",
      "Epoch 9/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01118901]]\n",
      "  Raw Prediction (y_pred): [[-0.00497439  0.01408449 -0.01399633  0.00470027 -0.02310823  0.02333442]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16581755 0.16900815 0.16432828 0.16742956 0.16283774 0.17057872]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16581755  0.16900815  0.16432828  0.16742956 -0.83716226  0.17057872]]\n",
      "  Gradient Wy (dWy): [[ 0.00185533]\n",
      " [ 0.00189103]\n",
      " [ 0.00183867]\n",
      " [ 0.00187337]\n",
      " [-0.00936701]\n",
      " [ 0.00190861]]\n",
      "  Gradient Wh (dWh): [[9.69799733e-06]]\n",
      "  Gradient Wx (dWx): [[0.00086674 0.         0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00086674]]\n",
      "  Gradient by (dby): [[ 0.16581755  0.16900815  0.16432828  0.16742956 -0.83716226  0.17057872]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00668327]]\n",
      "  Raw Prediction (y_pred): [[-0.02120699 -0.00308553 -0.03039519 -0.01231734  0.06060567  0.00637535]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16309785 0.16608037 0.16160614 0.1645542  0.17700235 0.16765909]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16309785  0.16608037 -0.83839386  0.1645542   0.17700235  0.16765909]]\n",
      "  Gradient Wy (dWy): [[-0.00109003]\n",
      " [-0.00110996]\n",
      " [ 0.00560321]\n",
      " [-0.00109976]\n",
      " [-0.00118295]\n",
      " [-0.00112051]]\n",
      "  Gradient Wh (dWh): [[-1.77489765e-05]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00265573 0.        ]]\n",
      "  Gradient bh (dbh): [[0.00265573]]\n",
      "  Gradient by (dby): [[ 0.16309785  0.16608037 -0.83839386  0.1645542   0.17700235  0.16765909]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.02034844]]\n",
      "  Raw Prediction (y_pred): [[-0.03725044 -0.01990004  0.05348314 -0.0289834   0.04289324 -0.01031566]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16047439 0.16328298 0.17571581 0.16180654 0.17386481 0.16485547]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83952561  0.16328298  0.17571581  0.16180654  0.17386481  0.16485547]]\n",
      "  Gradient Wy (dWy): [[ 0.01708303]\n",
      " [-0.00332255]\n",
      " [-0.00357554]\n",
      " [-0.00329251]\n",
      " [-0.00353788]\n",
      " [-0.00335455]]\n",
      "  Gradient Wh (dWh): [[-0.00040946]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.02012255 0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.02012255]]\n",
      "  Gradient by (dby): [[-0.83952561  0.16328298  0.17571581  0.16180654  0.17386481  0.16485547]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.00865036]]\n",
      "  Raw Prediction (y_pred): [[ 0.04612061 -0.03578888  0.03583996 -0.04471577  0.02553403 -0.02695884]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.1744152  0.16069837 0.17263128 0.15927022 0.17086129 0.16212363]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.1744152   0.16069837  0.17263128 -0.84072978  0.17086129  0.16212363]]\n",
      "  Gradient Wy (dWy): [[ 0.00150875]\n",
      " [ 0.0013901 ]\n",
      " [ 0.00149332]\n",
      " [-0.00727262]\n",
      " [ 0.00147801]\n",
      " [ 0.00140243]]\n",
      "  Gradient Wh (dWh): [[-0.00013362]]\n",
      "  Gradient Wx (dWx): [[-0.01544639  0.          0.          0.          0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01544639]]\n",
      "  Gradient by (dby): [[ 0.1744152   0.16069837  0.17263128 -0.84072978  0.17086129  0.16212363]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01643279]]\n",
      "  Raw Prediction (y_pred): [[ 0.02851122 -0.05174125  0.01855711  0.03949126  0.00845472 -0.04321398]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.1713811  0.15816476 0.16968362 0.17327324 0.16797804 0.15951924]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.1713811  -0.84183524  0.16968362  0.17327324  0.16797804  0.15951924]]\n",
      "  Gradient Wy (dWy): [[ 0.00281627]\n",
      " [-0.0138337 ]\n",
      " [ 0.00278838]\n",
      " [ 0.00284736]\n",
      " [ 0.00276035]\n",
      " [ 0.00262135]]\n",
      "  Gradient Wh (dWh): [[-0.00024213]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.01473482  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01473482]]\n",
      "  Gradient by (dby): [[ 0.1713811  -0.84183524  0.16968362  0.17327324  0.16797804  0.15951924]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00228093]]\n",
      "  Raw Prediction (y_pred): [[ 0.01177427  0.03215376  0.00163369  0.02185736 -0.00836193 -0.05906536]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16856841 0.17203899 0.16686766 0.17027669 0.16520802 0.15704023]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16856841  0.17203899  0.16686766  0.17027669  0.16520802 -0.84295977]]\n",
      "  Gradient Wy (dWy): [[-0.00038449]\n",
      " [-0.00039241]\n",
      " [-0.00038061]\n",
      " [-0.00038839]\n",
      " [-0.00037683]\n",
      " [ 0.00192273]]\n",
      "  Gradient Wh (dWh): [[-1.45077148e-05]]\n",
      "  Gradient Wx (dWx): [[0.         0.00636043 0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00636043]]\n",
      "  Gradient by (dby): [[ 0.16856841  0.17203899  0.16686766  0.17027669  0.16520802 -0.84295977]]\n",
      "\n",
      "Epoch 9 Loss: [10.99973937]\n",
      "Epoch 10/10\n",
      "\n",
      "Time Step 1/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01266434]]\n",
      "  Raw Prediction (y_pred): [[-0.00540616  0.01519893 -0.01509214  0.00507129 -0.02487083  0.02514445]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16574244 0.169193   0.16414481 0.16748812 0.16254751 0.17088411]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16574244  0.169193    0.16414481  0.16748812 -0.83745249  0.17088411]]\n",
      "  Gradient Wy (dWy): [[ 0.00209902]\n",
      " [ 0.00214272]\n",
      " [ 0.00207879]\n",
      " [ 0.00212113]\n",
      " [-0.01060578]\n",
      " [ 0.00216413]]\n",
      "  Gradient Wh (dWh): [[-1.88744849e-06]]\n",
      "  Gradient Wx (dWx): [[-0.00014904  0.          0.          0.          0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.00014904]]\n",
      "  Gradient by (dby): [[ 0.16574244  0.169193    0.16414481  0.16748812 -0.83745249  0.17088411]]\n",
      "\n",
      "Time Step 2/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00682234]]\n",
      "  Raw Prediction (y_pred): [[-0.02155716 -0.00204377 -0.03145438 -0.0119912   0.05885155  0.00817044]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16304244 0.1662552  0.16143674 0.16460959 0.17669397 0.16796207]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16304244  0.1662552  -0.83856326  0.16460959  0.17669397  0.16796207]]\n",
      "  Gradient Wy (dWy): [[-0.00111233]\n",
      " [-0.00113425]\n",
      " [ 0.00572097]\n",
      " [-0.00112302]\n",
      " [-0.00120547]\n",
      " [-0.0011459 ]]\n",
      "  Gradient Wh (dWh): [[-2.3745297e-05]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00348052 0.        ]]\n",
      "  Gradient bh (dbh): [[0.00348052]]\n",
      "  Gradient by (dby): [[ 0.16304244  0.1662552  -0.83856326  0.16460959  0.17669397  0.16796207]]\n",
      "\n",
      "Time Step 3/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.02232427]]\n",
      "  Raw Prediction (y_pred): [[-0.03752508 -0.01892694  0.05245838 -0.02870206  0.04115058 -0.00853514]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16043489 0.1634466  0.17554083 0.16185668 0.17356703 0.16515396]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[-0.83956511  0.1634466   0.17554083  0.16185668  0.17356703  0.16515396]]\n",
      "  Gradient Wy (dWy): [[ 0.01874268]\n",
      " [-0.00364883]\n",
      " [-0.00391882]\n",
      " [-0.00361333]\n",
      " [-0.00387476]\n",
      " [-0.00368694]]\n",
      "  Gradient Wh (dWh): [[-0.0004985]]\n",
      "  Gradient Wx (dWx): [[0.         0.         0.02233001 0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.02233001]]\n",
      "  Gradient by (dby): [[-0.83956511  0.1634466   0.17554083  0.16185668  0.17356703  0.16515396]]\n",
      "\n",
      "Time Step 4/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01001515]]\n",
      "  Raw Prediction (y_pred): [[ 0.04570945 -0.03473208  0.03479864 -0.04436437  0.02386191 -0.02523754]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17434902 0.16087338 0.17245707 0.15933124 0.17058123 0.16240807]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17434902  0.16087338  0.17245707 -0.84066876  0.17058123  0.16240807]]\n",
      "  Gradient Wy (dWy): [[ 0.00174613]\n",
      " [ 0.00161117]\n",
      " [ 0.00172718]\n",
      " [-0.00841943]\n",
      " [ 0.0017084 ]\n",
      " [ 0.00162654]]\n",
      "  Gradient Wh (dWh): [[-0.00016241]]\n",
      "  Gradient Wx (dWx): [[-0.01621626  0.          0.          0.          0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01621626]]\n",
      "  Gradient by (dby): [[ 0.17434902  0.16087338  0.17245707 -0.84066876  0.17058123  0.16240807]]\n",
      "\n",
      "Time Step 5/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[0.01780563]]\n",
      "  Raw Prediction (y_pred): [[ 0.02808744 -0.05069035  0.01752652  0.03984551  0.00681921 -0.04152431]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.17131268 0.15833493 0.16951298 0.17333887 0.16770763 0.15979291]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.17131268 -0.84166507  0.16951298  0.17333887  0.16770763  0.15979291]]\n",
      "  Gradient Wy (dWy): [[ 0.00305033]\n",
      " [-0.01498638]\n",
      " [ 0.00301829]\n",
      " [ 0.00308641]\n",
      " [ 0.00298614]\n",
      " [ 0.00284521]]\n",
      "  Gradient Wh (dWh): [[-0.00028974]]\n",
      "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.01627214  0.          0.        ]]\n",
      "  Gradient bh (dbh): [[-0.01627214]]\n",
      "  Gradient by (dby): [[ 0.17131268 -0.84166507  0.16951298  0.17333887  0.16770763  0.15979291]]\n",
      "\n",
      "Time Step 6/6\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00286259]]\n",
      "  Raw Prediction (y_pred): [[ 0.01144881  0.03312518  0.00064158  0.02215548 -0.00999617 -0.05738517]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.16851491 0.17220758 0.16670353 0.17032883 0.16493958 0.15730557]]\n",
      "Backward Propagation:\n",
      "  Gradient of Loss wrt Output (dy): [[ 0.16851491  0.17220758  0.16670353  0.17032883  0.16493958 -0.84269443]]\n",
      "  Gradient Wy (dWy): [[-0.00048239]\n",
      " [-0.00049296]\n",
      " [-0.0004772 ]\n",
      " [-0.00048758]\n",
      " [-0.00047215]\n",
      " [ 0.00241229]]\n",
      "  Gradient Wh (dWh): [[-1.93102137e-05]]\n",
      "  Gradient Wx (dWx): [[0.         0.00674571 0.         0.         0.         0.        ]]\n",
      "  Gradient bh (dbh): [[0.00674571]]\n",
      "  Gradient by (dby): [[ 0.16851491  0.17220758  0.16670353  0.17032883  0.16493958 -0.84269443]]\n",
      "\n",
      "Epoch 10 Loss: [10.99967164]\n",
      "\n",
      "Testing the RNN Model:\n",
      "Input: c, Predicted: t\n",
      "Input: r, Predicted: t\n",
      "Input: i, Predicted: t\n",
      "Input: c, Predicted: t\n",
      "Input: k, Predicted: t\n",
      "Input: e, Predicted: t\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "word = \"cricket\"\n",
    "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
    "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
    "encoded_word = [char_to_int[ch] for ch in word]\n",
    "\n",
    "print(\"Encoded Word using One-Hot Encoding:\", encoded_word, \"\\n\")\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 1  # Single neuron\n",
    "input_size = len(char_to_int)\n",
    "output_size = len(char_to_int)\n",
    "learning_rate = 0.1\n",
    "epochs = 10  # Fewer epochs for demonstration\n",
    "\n",
    "# Model parameters\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN step function\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "\n",
    "    for t in range(len(encoded_word) - 1):\n",
    "        print(f\"\\nTime Step {t + 1}/{len(encoded_word) - 1}\")\n",
    "\n",
    "        # Input preparation\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_word[t + 1]  # Target character index\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "\n",
    "        # Print forward propagation details\n",
    "        print(\"Forward Propagation:\")\n",
    "        print(f\"  Input (x_t): {x_t.T}\")\n",
    "        print(f\"  Hidden State (h_prev): {h_prev.T}\")\n",
    "        print(f\"  Raw Prediction (y_pred): {y_pred.T}\")\n",
    "        print(f\"  Softmax Prediction (y_pred_softmax): {y_pred_softmax.T}\")\n",
    "\n",
    "        # Loss calculation\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
    "        dWx = np.dot(dh, x_t.T)\n",
    "        dWh = np.dot(dh, h_prev.T)\n",
    "        dbh = dh\n",
    "\n",
    "        # Print backward propagation details\n",
    "        print(\"Backward Propagation:\")\n",
    "        print(f\"  Gradient of Loss wrt Output (dy): {dy.T}\")\n",
    "        print(f\"  Gradient Wy (dWy): {dWy}\")\n",
    "        print(f\"  Gradient Wh (dWh): {dWh}\")\n",
    "        print(f\"  Gradient Wx (dWx): {dWx}\")\n",
    "        print(f\"  Gradient bh (dbh): {dbh.T}\")\n",
    "        print(f\"  Gradient by (dby): {dby.T}\")\n",
    "\n",
    "        # Update parameters\n",
    "        Wy -= learning_rate * dWy\n",
    "        by -= learning_rate * dby\n",
    "        Wx -= learning_rate * dWx\n",
    "        Wh -= learning_rate * dWh\n",
    "        bh -= learning_rate * dbh\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} Loss: {loss}\")\n",
    "\n",
    "# Testing (prediction after training)\n",
    "print(\"\\nTesting the RNN Model:\")\n",
    "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
    "for t in range(len(encoded_word) - 1):\n",
    "    x_t = np.zeros((input_size, 1))\n",
    "    x_t[encoded_word[t]] = 1  # One-hot encoding\n",
    "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "    next_char = int_to_char[np.argmax(y_pred)]\n",
    "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaded981-bee7-4829-a871-be8e0bc8d41b",
   "metadata": {},
   "source": [
    "## Next Word Prediction in Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c9551049-cf35-40c6-8992-3d5a339e6fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence using One-Hot Encoding: [0, 7, 1, 3, 4, 6, 8, 5, 2] \n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n",
      "\n",
      "Epoch 2/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n",
      "\n",
      "Epoch 3/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n",
      "\n",
      "Epoch 4/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n",
      "\n",
      "Epoch 5/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n",
      "\n",
      "Epoch 6/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n",
      "\n",
      "Epoch 7/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n",
      "\n",
      "Epoch 8/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n",
      "\n",
      "Epoch 9/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n",
      "\n",
      "Epoch 10/10\n",
      "\n",
      "Time Step 1/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00351841  0.00639902 -0.00930101 -0.0005545 ]]\n",
      "  Raw Prediction (y_pred): [[-3.20315762e-04 -7.53254354e-05 -1.00965771e-04  1.64770052e-04\n",
      "   1.81647110e-05  1.94708226e-04  1.24308224e-04 -1.45086014e-05\n",
      "  -3.00179777e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107934 0.11110656 0.11110371 0.11113324 0.11111695 0.11113657\n",
      "  0.11112874 0.11111332 0.11108158]]\n",
      "\n",
      "Time Step 2/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00895977  0.0030413  -0.00274682  0.01586492]]\n",
      "  Raw Prediction (y_pred): [[-1.98527715e-04 -7.94752191e-05 -4.74081440e-05  3.98484725e-05\n",
      "  -1.11895958e-04 -2.43639659e-04  2.00445602e-04 -1.41417413e-04\n",
      "   2.01372833e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11109375 0.11110698 0.11111054 0.11112024 0.11110338 0.11108874\n",
      "  0.11113808 0.1111001  0.11113819]]\n",
      "\n",
      "Time Step 3/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00174025  0.00623856 -0.01283669 -0.01285805]]\n",
      "  Raw Prediction (y_pred): [[-2.57652723e-04 -6.57820224e-05  5.31467361e-05  1.00907467e-04\n",
      "   3.32336174e-04  5.97324604e-04  3.58734026e-04  4.70035450e-05\n",
      "  -5.50097148e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107488 0.11109619 0.11110941 0.11111471 0.11114043 0.11116989\n",
      "  0.11114337 0.11110872 0.1110424 ]]\n",
      "\n",
      "Time Step 4/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[-0.00537305 -0.012168    0.00171017 -0.01758288]]\n",
      "  Raw Prediction (y_pred): [[ 3.25964002e-04  2.14193247e-04  1.83282111e-04 -1.62564400e-04\n",
      "   3.95987330e-04  3.70705456e-04  1.86677946e-04  1.25677717e-04\n",
      "  -6.54983425e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11112789 0.11111547 0.11111204 0.11107362 0.11113567 0.11113286\n",
      "  0.11111241 0.11110564 0.1110844 ]]\n",
      "\n",
      "Time Step 5/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.01877592 -0.01182674 -0.00900453 -0.01058135]]\n",
      "  Raw Prediction (y_pred): [[-2.32396920e-04  3.03169790e-04 -3.30587910e-04  2.65861016e-04\n",
      "   1.18241445e-05  2.20428113e-04 -7.86159195e-05  1.24402461e-04\n",
      "  -2.73698821e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11108516 0.11114467 0.11107425 0.11114052 0.11111229 0.11113547\n",
      "  0.11110225 0.1111248  0.11108057]]\n",
      "\n",
      "Time Step 6/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00303453 -0.01183697  0.00011575  0.0062777 ]]\n",
      "  Raw Prediction (y_pred): [[-3.17382162e-05  2.20094389e-04 -1.85363370e-04  7.10326006e-05\n",
      "  -1.10547116e-04 -2.35275713e-04 -1.75324187e-05 -1.48459452e-05\n",
      "   2.55005128e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110819 0.11113617 0.11109112 0.11111961 0.11109943 0.11108558\n",
      "  0.11110977 0.11111007 0.11114005]]\n",
      "\n",
      "Time Step 7/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "  Hidden State (h_prev): [[ 0.00294335 -0.00900969 -0.00778176  0.00958735]]\n",
      "  Raw Prediction (y_pred): [[-3.10580183e-04  1.93453227e-04 -2.85580203e-04  2.04364884e-04\n",
      "  -1.02293735e-04 -1.49674165e-04  1.58177433e-04 -5.69617079e-05\n",
      "   1.10957018e-04]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11107954 0.11113555 0.11108232 0.11113676 0.11110268 0.11109742\n",
      "  0.11113163 0.11110772 0.11112638]]\n",
      "\n",
      "Time Step 8/8\n",
      "Forward Propagation:\n",
      "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "  Hidden State (h_prev): [[ 0.00098462  0.00368825 -0.00231767  0.00173918]]\n",
      "  Raw Prediction (y_pred): [[-1.09496819e-04 -5.80235080e-05 -3.08499751e-05  5.05915671e-05\n",
      "  -3.50343591e-05  1.30347714e-05  1.18951273e-05 -1.92270151e-05\n",
      "  -7.30262623e-05]]\n",
      "  Softmax Prediction (y_pred_softmax): [[0.11110203 0.11110775 0.11111077 0.11111982 0.11111031 0.11111565\n",
      "  0.11111552 0.11111206 0.11110609]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "words = sentence.split()\n",
    "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "encoded_sentence = [word_to_int[word] for word in words]\n",
    "\n",
    "print(\"Encoded Sentence using One-Hot Encoding:\", encoded_sentence, \"\\n\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(word_to_int)  # Number of unique words\n",
    "output_size = len(word_to_int)\n",
    "hidden_size = 4  # Number of hidden neurons\n",
    "learning_rate = 0.01\n",
    "epochs = 10  # Fewer epochs for demonstration\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
    "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
    "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# RNN step function\n",
    "def rnn_step_forward(x, h_prev):\n",
    "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
    "    y = np.dot(Wy, h_next) + by\n",
    "    return h_next, y\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    loss = 0\n",
    "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "\n",
    "    for t in range(len(encoded_sentence) - 1):\n",
    "        print(f\"\\nTime Step {t + 1}/{len(encoded_sentence) - 1}\")\n",
    "\n",
    "        # Input preparation\n",
    "        x_t = np.zeros((input_size, 1))\n",
    "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
    "        y_true = encoded_sentence[t + 1]  # Target word index\n",
    "\n",
    "        # Forward pass\n",
    "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
    "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
    "\n",
    "        # Print forward propagation details\n",
    "        print(\"Forward Propagation:\")\n",
    "        print(f\"  Input (x_t): {x_t.T}\")\n",
    "        print(f\"  Hidden State (h_prev): {h_prev.T}\")\n",
    "        print(f\"  Raw Prediction (y_pred): {y_pred.T}\")\n",
    "        print(f\"  Softmax Prediction (y_pred_softmax): {y_pred_softmax.T}\")\n",
    "\n",
    "        # Loss calculation\n",
    "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass\n",
    "        dy = y_pred_softmax\n",
    "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
    "\n",
    "        dWy = np.dot(dy, h_prev.T)\n",
    "        dby = dy\n",
    "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86e1d5-1452-4bcc-9ea8-1067af9bb728",
   "metadata": {},
   "source": [
    "# **Q#4 - Self Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "994df3d9-da35-45fd-a333-2768fb80dde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights (Softmax Scores):\n",
      "Word: The\n",
      "Attention to: {'The': 0.25, 'weather': 0.15, 'today': 0.12, 'is': 0.13, 'sunny': 0.17, 'and': 0.13, 'warm': 0.05}\n",
      "\n",
      "Word: weather\n",
      "Attention to: {'The': 0.37, 'weather': 0.27, 'today': 0.03, 'is': 0.05, 'sunny': 0.09, 'and': 0.17, 'warm': 0.02}\n",
      "\n",
      "Word: today\n",
      "Attention to: {'The': 0.08, 'weather': 0.11, 'today': 0.39, 'is': 0.13, 'sunny': 0.13, 'and': 0.09, 'warm': 0.08}\n",
      "\n",
      "Word: is\n",
      "Attention to: {'The': 0.4, 'weather': 0.29, 'today': 0.1, 'is': 0.01, 'sunny': 0.09, 'and': 0.06, 'warm': 0.06}\n",
      "\n",
      "Word: sunny\n",
      "Attention to: {'The': 0.67, 'weather': 0.14, 'today': 0.05, 'is': 0.03, 'sunny': 0.05, 'and': 0.04, 'warm': 0.02}\n",
      "\n",
      "Word: and\n",
      "Attention to: {'The': 0.17, 'weather': 0.22, 'today': 0.07, 'is': 0.07, 'sunny': 0.19, 'and': 0.23, 'warm': 0.04}\n",
      "\n",
      "Word: warm\n",
      "Attention to: {'The': 0.44, 'weather': 0.08, 'today': 0.25, 'is': 0.05, 'sunny': 0.09, 'and': 0.06, 'warm': 0.03}\n",
      "\n",
      "Output of Self-Attention:\n",
      "Word: The, Output: [ 0.68 -0.51 -0.37  0.    0.37 -0.23 -0.34]\n",
      "Word: weather, Output: [ 0.44 -0.42 -0.3   0.32  0.61  0.04 -0.84]\n",
      "Word: today, Output: [ 0.83 -0.79 -0.31 -0.66 -0.22 -0.58 -0.07]\n",
      "Word: is, Output: [ 0.55 -0.59 -0.16  0.21  0.43 -0.05 -0.92]\n",
      "Word: sunny, Output: [ 0.24 -0.38  0.18  0.52  0.59 -0.21 -0.65]\n",
      "Word: and, Output: [ 0.7  -0.49 -0.45 -0.    0.58 -0.14 -0.54]\n",
      "Word: warm, Output: [ 0.47 -0.57  0.14 -0.1   0.22 -0.55 -0.28]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "sentence = \"The weather today is sunny and warm\"\n",
    "sequence = sentence.split()\n",
    "\n",
    "# Token-to-index mapping\n",
    "token_to_index = {word: i for i, word in enumerate(sequence)}\n",
    "index_to_token = {i: word for word, i in token_to_index.items()}\n",
    "\n",
    "# Create one-hot encoding for the sequence\n",
    "sequence_length = len(sequence)\n",
    "vocab_size = len(sequence)\n",
    "one_hot_encoded = np.eye(vocab_size)[[token_to_index[word] for word in sequence]]\n",
    "\n",
    "# Self-Attention Mechanism\n",
    "def self_attention(inputs, weights_query, weights_key, weights_value):\n",
    "   \n",
    "    queries = np.dot(inputs, weights_query)  # Shape: (seq_length, d_k)\n",
    "    keys = np.dot(inputs, weights_key)      # Shape: (seq_length, d_k)\n",
    "    values = np.dot(inputs, weights_value)  # Shape: (seq_length, d_v)\n",
    "\n",
    "    # Compute scaled dot-product attention scores\n",
    "    scores = np.dot(queries, keys.T) / np.sqrt(keys.shape[1])  # Shape: (seq_length, seq_length)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)  # Softmax\n",
    "\n",
    "    # Weighted sum of values\n",
    "    output = np.dot(attention_weights, values)  # Shape: (seq_length, d_v)\n",
    "\n",
    "    return attention_weights, output\n",
    "\n",
    "# Initialize random weights for query, key, and value\n",
    "embedding_dim = sequence_length  # Use same dimension as the sequence length for simplicity\n",
    "weights_query = np.random.randn(embedding_dim, embedding_dim)\n",
    "weights_key = np.random.randn(embedding_dim, embedding_dim)\n",
    "weights_value = np.random.randn(embedding_dim, embedding_dim)\n",
    "\n",
    "# Compute self-attention\n",
    "attention_weights, output = self_attention(one_hot_encoded, weights_query, weights_key, weights_value)\n",
    "\n",
    "# Display results\n",
    "print(\"Attention Weights (Softmax Scores):\")\n",
    "for i, token in enumerate(sequence):\n",
    "    print(f\"Word: {token}\")\n",
    "    attention_scores = {index_to_token[j]: round(attention_weights[i, j], 2) for j in range(sequence_length)}\n",
    "    print(f\"Attention to: {attention_scores}\\n\")\n",
    "\n",
    "print(\"Output of Self-Attention:\")\n",
    "for i, token in enumerate(sequence):\n",
    "    print(f\"Word: {token}, Output: {np.round(output[i], 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d5859863-e162-4d0e-ba9b-30043f8cd9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: The quick brown fox jumps over the lazy dog\n",
      "Encoded Sentence: [0, 7, 1, 3, 4, 6, 8, 5, 2] \n",
      "\n",
      "Self-Attention Mechanism:\n",
      "\n",
      "Sequence Embeddings (Input):\n",
      "[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "   1.57921282  0.76743473]\n",
      " [-0.83921752 -0.30921238  0.33126343  0.97554513 -0.47917424 -0.18565898\n",
      "  -1.10633497 -1.19620662]\n",
      " [-0.46947439  0.54256004 -0.46341769 -0.46572975  0.24196227 -1.91328024\n",
      "  -1.72491783 -0.56228753]\n",
      " [-0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
      "  -0.60170661  1.85227818]\n",
      " [-0.01349722 -1.05771093  0.82254491 -1.22084365  0.2088636  -1.95967012\n",
      "  -1.32818605  0.19686124]\n",
      " [ 0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922    0.61167629\n",
      "   1.03099952  0.93128012]\n",
      " [ 0.81252582  1.35624003 -0.07201012  1.0035329   0.36163603 -0.64511975\n",
      "   0.36139561  1.53803657]\n",
      " [ 0.73846658  0.17136828 -0.11564828 -0.3011037  -1.47852199 -0.71984421\n",
      "  -0.46063877  1.05712223]\n",
      " [-1.01283112  0.31424733 -0.90802408 -1.4123037   1.46564877 -0.2257763\n",
      "   0.0675282  -1.42474819]]\n",
      "\n",
      "Query Matrix:\n",
      "[[ 1.46980507 -1.03600977 -0.80628069 -1.52267899 -2.39599583  3.18168486\n",
      "  -2.945073   -3.07718038]\n",
      " [-0.71686864 -0.04138778  1.21355483  0.89179012 -0.93187049 -2.85006322\n",
      "   2.07865128 -1.05554308]\n",
      " [-1.56953771 -3.66363421  1.26102058  0.83813456  0.21877208 -1.61076114\n",
      "   0.36966748  0.82357732]\n",
      " [ 0.50405745 -2.65143613 -3.3128751  -0.93027239 -2.32612782  0.27104413\n",
      "   0.08625936  5.5097923 ]\n",
      " [-1.74717337 -3.92939194 -2.85173523  2.94118094  1.1376885   0.2373488\n",
      "  -2.37929471 -0.96705638]\n",
      " [ 1.14363855 -0.19940866 -4.86857184  0.28522696  1.19300085  2.87261379\n",
      "  -1.45705061 -0.09046757]\n",
      " [ 0.35023764 -0.56531856 -1.19789526 -0.87282093 -3.7018412   0.92504787\n",
      "  -2.20286448 -0.18428634]\n",
      " [-0.15592639 -1.40521322 -6.62981621  0.71978753 -1.31602506  0.30073159\n",
      "   1.87179909  0.39666144]\n",
      " [-0.30847021 -1.61527496  8.13436281 -1.54812011  4.23264371  0.66468802\n",
      "  -0.79242298  2.12321079]]\n",
      "\n",
      "Key Matrix:\n",
      "[[-2.4840771   1.08881005  3.68000371 -2.40659043  0.57249945 -0.60969403\n",
      "  -0.82718866  3.5265519 ]\n",
      " [-0.49285828  1.66485063  0.67175784 -0.09044129  3.12049862  0.85855073\n",
      "   2.16949726  1.73964977]\n",
      " [ 1.45861489 -3.39333626 -2.35099266 -4.91818497 -0.3950089   3.38471286\n",
      "   3.91313062 -2.64386081]\n",
      " [ 1.54635178 -2.41790713  0.1653916   1.49156547 -3.11530758  0.17241238\n",
      "   3.10557165 -3.37512239]\n",
      " [ 0.62848878 -5.79190232 -0.13764482 -3.5008296  -0.48808469  3.3473029\n",
      "   0.53414062 -4.27975873]\n",
      " [-0.42932364 -0.74155865  3.19859393  4.48944567 -1.86962107 -2.25371257\n",
      "  -2.94511848 -1.51130177]\n",
      " [-1.06170996 -1.29644     0.41142576 -5.04665541 -0.3718256   2.21030604\n",
      "   0.66343098  1.01555779]\n",
      " [ 0.5209645  -1.9454029   1.47679056 -1.11654174 -1.12909971  1.81000996\n",
      "  -0.62490111 -3.6244079 ]\n",
      " [ 2.18628811 -0.36042156 -4.57381451  0.27566936 -2.17328067 -1.8715916\n",
      "   2.16178796 -0.56344152]]\n",
      "\n",
      "Value Matrix:\n",
      "[[ 2.82237615  1.5268746  -2.0282805   0.47086253  2.988742    0.88920701\n",
      "  -4.9891794  -2.16208922]\n",
      " [-4.51928965  0.05394407 -1.44053505 -4.6663449   0.88435252 -0.46824618\n",
      "   5.14353231  0.4850072 ]\n",
      " [-1.54088502  1.19086724  1.47420499 -2.93647597 -0.90798011  0.28018068\n",
      "   4.80116867  3.6846499 ]\n",
      " [ 2.06275769 -0.32586978  0.20225232 -0.69759245 -3.50444614  2.24140508\n",
      "  -5.93310585 -0.18547712]\n",
      " [-1.14197762 -6.19533136  0.12407822 -2.46300856  0.71483276 -3.73048628\n",
      "   3.95509461 -0.50065526]\n",
      " [ 1.63058178 -6.9665107  -2.84544304  0.56977706  0.83163954 -1.65931963\n",
      "  -3.19106746 -4.19745051]\n",
      " [ 3.68562612  5.68704534  2.4302126   2.5027518  -1.32679083  1.50269471\n",
      "  -6.25867648  0.75936445]\n",
      " [ 1.61310894  0.88521163 -1.54899105  0.12344986  0.9946352  -1.14301517\n",
      "  -0.9393221  -1.01908378]\n",
      " [-0.27235748 -0.57367539  1.74317039  0.42924295 -0.62960206  2.39360784\n",
      "   3.44031677  3.70047432]]\n",
      "\n",
      "Attention Weights (Softmax of Scores):\n",
      "[[1.15964209e-08 3.32695667e-09 1.20280984e-02 1.89168027e-04\n",
      "  9.68090580e-01 5.01031560e-06 1.05707809e-04 1.95761948e-02\n",
      "  5.22552918e-06]\n",
      " [5.40837451e-03 3.20468512e-03 5.79836393e-04 5.62082516e-01\n",
      "  4.92932794e-04 3.82398339e-01 2.54019215e-04 3.94284127e-03\n",
      "  4.16364554e-02]\n",
      " [8.32075061e-02 3.15082565e-03 2.94263609e-03 5.96752818e-02\n",
      "  1.72385030e-01 6.54884534e-01 9.64779684e-03 1.24111883e-02\n",
      "  1.69520148e-03]\n",
      " [4.40339743e-03 2.76551772e-04 3.58647945e-02 1.50421093e-04\n",
      "  5.51479622e-04 2.09681171e-06 1.30252079e-01 5.12245540e-06\n",
      "  8.28494057e-01]\n",
      " [4.55829037e-06 6.44831097e-05 9.39555569e-04 9.22651509e-03\n",
      "  5.93532845e-01 3.73501272e-01 5.26487060e-05 1.02210211e-02\n",
      "  1.24571016e-02]\n",
      " [9.26389705e-07 1.31164921e-03 6.37765724e-01 2.92036870e-04\n",
      "  7.67274349e-02 2.61138152e-06 2.45275923e-03 1.26054050e-03\n",
      "  2.80186317e-01]\n",
      " [1.30150671e-03 1.62797282e-05 7.07867164e-02 6.79100939e-02\n",
      "  4.50038355e-01 3.64503846e-02 7.19723079e-02 1.64660627e-01\n",
      "  1.36863729e-01]\n",
      " [9.73771495e-11 2.75537924e-07 1.22904430e-02 1.40766503e-04\n",
      "  2.99751465e-05 1.22584748e-09 1.26227184e-06 1.08337824e-07\n",
      "  9.87537168e-01]\n",
      " [9.99726034e-01 2.05812837e-04 1.51212343e-09 2.07440631e-10\n",
      "  1.13464080e-06 9.47023782e-06 5.53748802e-05 2.17165814e-06\n",
      "  3.81181616e-15]]\n",
      "\n",
      "Self-Attention Output:\n",
      "[[-1.09170658 -5.96548735  0.10781746 -2.41718125  0.69977049 -3.62986743\n",
      "   3.86646841 -0.46026663]\n",
      " [ 1.77825344 -2.86003136 -0.92199107 -0.17055241 -1.65557587  0.72250203\n",
      "  -4.42297125 -1.5673515 ]\n",
      " [ 1.26526479 -5.45408569 -1.99177053 -0.05083531  0.70600257 -1.5182793\n",
      "  -2.21305261 -3.01279939]\n",
      " [ 0.2100208   0.3114357   1.80437386  0.57561925 -0.71372425  2.19092417\n",
      "   2.18799961  3.28717457]\n",
      " [-0.03818221 -6.27880972 -0.97997252 -1.25181743  0.70403219 -2.79479292\n",
      "   1.13861483 -1.82741531]\n",
      " [-1.14090061  0.13843004  1.44029855 -1.9415242  -0.70249624  0.56539697\n",
      "   4.31788056  3.34949866]\n",
      " [ 0.07371194 -2.50135895  0.22597527 -1.08317598  0.03577518 -1.31860706\n",
      "   1.45989393  0.26042062]\n",
      " [-0.28764158 -0.55211378  1.73959888  0.38763263 -0.63338812  2.36742559\n",
      "   3.45572595  3.69960177]\n",
      " [ 2.82089453  1.52671123 -2.0279169   0.46991459  2.98804258  0.88892781\n",
      "  -4.98712827 -2.16139753]]\n",
      "\n",
      "Reconstructed Sequence (Using Attention Outputs): over The The quick over over over quick jumps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a user example: sequence of words\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "words = sentence.split()\n",
    "vocab = sorted(set(words))\n",
    "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "encoded_sentence = [word_to_int[word] for word in words]\n",
    "\n",
    "print(\"Input Sentence:\", sentence)\n",
    "print(\"Encoded Sentence:\", encoded_sentence, \"\\n\")\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_size = 8  # Size of embedding vectors\n",
    "sequence_length = len(words)  # Number of tokens\n",
    "d_k = embedding_size  # Dimension of the key vectors (commonly the same as embedding size)\n",
    "\n",
    "# Random embeddings for demonstration\n",
    "np.random.seed(42)  # Seed for reproducibility\n",
    "word_embeddings = np.random.randn(len(vocab), embedding_size)  # Embedding matrix\n",
    "sequence_embeddings = np.array([word_embeddings[i] for i in encoded_sentence])  # Input embeddings\n",
    "\n",
    "# Initialize Query, Key, and Value weights\n",
    "Wq = np.random.randn(embedding_size, d_k)\n",
    "Wk = np.random.randn(embedding_size, d_k)\n",
    "Wv = np.random.randn(embedding_size, embedding_size)\n",
    "\n",
    "# Compute Query, Key, and Value matrices\n",
    "queries = np.dot(sequence_embeddings, Wq)  # (sequence_length x d_k)\n",
    "keys = np.dot(sequence_embeddings, Wk)     # (sequence_length x d_k)\n",
    "values = np.dot(sequence_embeddings, Wv)   # (sequence_length x embedding_size)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Scale by sqrt(d_k)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)  # Softmax\n",
    "    output = np.dot(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Calculate self-attention\n",
    "attention_output, attention_weights = scaled_dot_product_attention(queries, keys, values)\n",
    "\n",
    "# Print results\n",
    "print(\"Self-Attention Mechanism:\\n\")\n",
    "print(\"Sequence Embeddings (Input):\")\n",
    "print(sequence_embeddings)\n",
    "print(\"\\nQuery Matrix:\")\n",
    "print(queries)\n",
    "print(\"\\nKey Matrix:\")\n",
    "print(keys)\n",
    "print(\"\\nValue Matrix:\")\n",
    "print(values)\n",
    "print(\"\\nAttention Weights (Softmax of Scores):\")\n",
    "print(attention_weights)\n",
    "print(\"\\nSelf-Attention Output:\")\n",
    "print(attention_output)\n",
    "\n",
    "# Reconstruct the sequence using the attention output\n",
    "reconstructed_sequence = [int_to_word[np.argmax(row)] for row in attention_output]\n",
    "print(\"\\nReconstructed Sequence (Using Attention Outputs):\", \" \".join(reconstructed_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b18f1b-adc1-4dba-bf56-7eee612c0653",
   "metadata": {},
   "source": [
    "# Q5 - Encoding Schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea535247-bba3-44b0-8359-630dc7065f84",
   "metadata": {},
   "source": [
    "## 1. Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab090e-d6ab-4ff6-98ac-58c9fa6b08a0",
   "metadata": {},
   "source": [
    "Label encoding assigns an integer to each category in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "94652229-df72-486f-a5b4-0592cef21dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [0 1 0 2]\n",
      "Original categories: ['Apple' 'Banana' 'Apple' 'Cherry']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data\n",
    "categories = ['Apple', 'Banana', 'Apple', 'Cherry']\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "labels = encoder.fit_transform(categories)\n",
    "\n",
    "print(\"Encoded labels:\", labels)\n",
    "print(\"Original categories:\", encoder.inverse_transform(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf76702-1fc0-4624-a5cb-cc5db8b57eff",
   "metadata": {},
   "source": [
    "##### Advantages:\n",
    "\n",
    "- Simple to implement.\n",
    "- Efficient for ordinal data.\n",
    "##### Disadvantages:\n",
    "\n",
    "- Imposes an artificial ordinal relationship between the categories (e.g., \"cat\" < \"dog\" < \"bird\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a7714-c472-4a40-b473-10ac2652441c",
   "metadata": {},
   "source": [
    "## 2. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a64a7d-b536-4f0f-806a-21c1d4da8c37",
   "metadata": {},
   "source": [
    "One-Hot Encoding is a method used to convert categorical data into a binary format that can be fed into machine learning models. It creates new binary columns (features), each representing one unique category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4fcbfa65-1773-438b-9daa-52562678b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category_Apple  Category_Banana  Category_Cherry\n",
      "0               1                0                0\n",
      "1               0                1                0\n",
      "2               1                0                0\n",
      "3               0                0                1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Category': ['Apple', 'Banana', 'Apple', 'Cherry']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-hot encoding\n",
    "encoded_df = pd.get_dummies(df, columns=['Category']).astype(int)\n",
    "print(encoded_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfef6f-44f1-4e35-a55d-190ca3d9dfdb",
   "metadata": {},
   "source": [
    "##### Advantages:\n",
    "\n",
    "- Simple and easy to implement.\n",
    "- Works well for nominal categorical variables.\n",
    "##### Disadvantages:\n",
    "\n",
    "- High dimensionality if the data has many unique categories.\n",
    "- Sparse representation, which can be inefficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1c4f67e5-374c-424b-b4ab-1eb3a85f6027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category-encodersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading category_encoders-2.6.4-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from category-encoders) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from category-encoders) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from category-encoders) (1.13.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from category-encoders) (0.14.2)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from category-encoders) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from category-encoders) (0.5.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category-encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category-encoders) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category-encoders) (3.5.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\nomic\\anaconda3\\lib\\site-packages (from statsmodels>=0.9.0->category-encoders) (24.1)\n",
      "Downloading category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.6.4\n"
     ]
    }
   ],
   "source": [
    "pip install category-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01973e39-6d4f-492a-9bc5-e12102f3faf0",
   "metadata": {},
   "source": [
    "## 3. Binary Encoding\n",
    "Binary Encoding is a data preprocessing technique that encodes categorical data into binary format. Each category is first assigned a unique integer value, which is then converted into its binary representation. The binary digits are then split into separate columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a0eee89b-f31d-4a05-9dc1-2e6b031068a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category_0  Category_1\n",
      "0           0           1\n",
      "1           1           0\n",
      "2           1           1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "# Sample data\n",
    "data = {'Category': ['Apple', 'Banana', 'Cherry']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Binary Encoding\n",
    "encoder = BinaryEncoder(cols=['Category'])\n",
    "binary_encoded = encoder.fit_transform(df)\n",
    "\n",
    "print(binary_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910bf5c-4e9f-4c37-89f0-0297fc9f41f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Advantages:\n",
    "\n",
    "- Reduces dimensionality compared to one-hot encoding.\n",
    "- Avoids introducing ordinal relationships.\n",
    "##### Disadvantages:\n",
    "\n",
    "- Binary code can still lead to a loss of interpretability.\n",
    "- Can result in collision for large sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e431d4-1b5f-4492-b8da-56c08d50e04d",
   "metadata": {},
   "source": [
    "## 4. Frequency Encoding\n",
    "Frequency encoding involves assigning each category the frequency of its occurrence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3665c1d3-810c-421d-8f24-75353c4ee23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Encoding: {'cat': 2, 'dog': 3, 'bird': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def frequency_encoding(data):\n",
    "    freq = Counter(data)\n",
    "    encoding = {val: freq[val] for val in data}\n",
    "    return encoding\n",
    "\n",
    "data = [\"cat\", \"dog\", \"bird\", \"dog\", \"cat\", \"dog\"]\n",
    "encoding = frequency_encoding(data)\n",
    "print(\"Frequency Encoding:\", encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ecbafe-9664-4f41-a3f9-dcc3681b0524",
   "metadata": {},
   "source": [
    "##### Advantages:\n",
    "- Simple and effective for large datasets.\n",
    "- Can work with categorical variables that have many unique values.\n",
    "##### Disadvantages:\n",
    "- Doesn't capture any ordinal information.\n",
    "- Might not work well if the frequencies are similar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b2a619-400c-4d71-af5b-8067dcefbdbd",
   "metadata": {},
   "source": [
    "## 5. Target Encoding\n",
    "Target encoding involves replacing a category with the mean of the target variable for that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0be73ef8-ba21-4342-b8e4-91fdee005c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding: {'bird': 1.0, 'cat': 1.0, 'dog': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def target_encoding(data, target):\n",
    "    df = pd.DataFrame({'data': data, 'target': target})\n",
    "    encoding = df.groupby('data')['target'].mean().to_dict()\n",
    "    return encoding\n",
    "\n",
    "data = [\"cat\", \"dog\", \"bird\", \"dog\", \"cat\", \"dog\"]\n",
    "target = [1, 0, 1, 0, 1, 0]\n",
    "encoding = target_encoding(data, target)\n",
    "print(\"Target Encoding:\", encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7dba7-45f9-4682-8af7-b59992470fda",
   "metadata": {},
   "source": [
    "##### Advantages:\n",
    "\n",
    "- Can improve model performance for categorical variables.\n",
    "- Reduces dimensionality.\n",
    "##### Disadvantages:\n",
    "\n",
    "- Risk of overfitting.\n",
    "- Needs careful handling of unseen categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19d016-507d-4726-a679-ae5947dff755",
   "metadata": {},
   "source": [
    "## 6. Hashing encoding\n",
    "Hashing encoding is used for high cardinality categorical variables, where we hash the categories into a fixed-size vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "08ad7717-655c-4c80-8da6-8d83d6522735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing Encoding: {'cat': 2, 'dog': 5, 'bird': 9}\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def hashing_encoding(data, num_bins=10):\n",
    "    return {val: int(hashlib.md5(val.encode('utf-8')).hexdigest(), 16) % num_bins for val in data}\n",
    "\n",
    "data = [\"cat\", \"dog\", \"bird\"]\n",
    "encoding = hashing_encoding(data)\n",
    "print(\"Hashing Encoding:\", encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3ae14-66ef-4237-86e1-8559b887351f",
   "metadata": {},
   "source": [
    "##### Advantages:\n",
    "\n",
    "- Reduces memory usage by mapping to a fixed number of buckets.\n",
    "- Effective for handling high cardinality data.\n",
    "##### Disadvantages:\n",
    "\n",
    "- Potential hash collisions.\n",
    "- Loss of interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306d852-ed8f-4686-890c-98527b8de5ae",
   "metadata": {},
   "source": [
    "## 7. Ordinal Encoding\n",
    "Ordinal encoding assigns an integer to each category, but maintains the inherent order of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "fdeae047-b857-4155-970a-7ff9cd4365fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal Encoding: [0, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "def ordinal_encoding(data, order):\n",
    "    encoding = {val: idx for idx, val in enumerate(order)}\n",
    "    return [encoding[val] for val in data]\n",
    "\n",
    "data = [\"small\", \"large\", \"medium\"]\n",
    "order = [\"small\", \"medium\", \"large\"]\n",
    "encoded_data = ordinal_encoding(data, order)\n",
    "print(\"Ordinal Encoding:\", encoded_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7155bc2d-5c68-4fdd-a28f-11e5eb9c201d",
   "metadata": {},
   "source": [
    "##### Advantages:\n",
    "\n",
    "- Works well for ordinal data.\n",
    "- Simple to implement.\n",
    "##### Disadvantages:\n",
    "\n",
    "- Limited to ordinal data, not useful for nominal categories.\n",
    "- May create unwanted relationships for non-ordinal data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b4bb5-f7fd-4f0d-b629-980e19993fb7",
   "metadata": {},
   "source": [
    "## 8. Count Encoding\n",
    "Count encoding assigns each category the number of times it appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3222b533-0b43-480f-90da-cb5fa91c9479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Encoding: {'cat': 2, 'dog': 3, 'bird': 1}\n"
     ]
    }
   ],
   "source": [
    "def count_encoding(data):\n",
    "    count = {val: data.count(val) for val in data}\n",
    "    return count\n",
    "\n",
    "data = [\"cat\", \"dog\", \"bird\", \"dog\", \"cat\", \"dog\"]\n",
    "encoding = count_encoding(data)\n",
    "print(\"Count Encoding:\", encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58fc532-cb8a-4404-b00c-68f8f711254c",
   "metadata": {},
   "source": [
    "##### Advantages:\n",
    "\n",
    "- Simple to compute.\n",
    "- Works well when frequency of categories is relevant.\n",
    "##### Disadvantages:\n",
    "\n",
    "- Might not be effective for datasets where the frequency distribution is close to uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038adbe-aae8-4f16-80da-95322960d170",
   "metadata": {},
   "source": [
    "## 9. Count Vectorizer & BoW\n",
    "CountVectorizer is a tool provided by the scikit-learn library to convert a collection of text documents into a matrix of token counts. It is used to implement the Bag of Words (BoW) model, where each document is represented as a vector of word counts.\n",
    "\n",
    "The Bag of Words (BoW) model is a simple and commonly used technique in Natural Language Processing (NLP) to represent text data in numerical form. It disregards grammar, word order, and context, focusing only on word occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43541b65-b633-41cb-b451-baf603cf5b48",
   "metadata": {},
   "source": [
    "### **How CountVectorizer Works**\n",
    "1. **Tokenization**: Splits the text into individual words (tokens).\n",
    "2. **Vocabulary Creation**: Builds a vocabulary of unique words from the corpus.\n",
    "3. **Vectorization**:\n",
    "   - Counts the occurrences of each word in the vocabulary for each document.\n",
    "   - Represents documents as vectors based on these counts.\n",
    "\n",
    "\n",
    "### **How Bag of Words Works**\n",
    "\n",
    "1. **Text Preprocessing**:\n",
    "   - Tokenize the text into words (e.g., split sentences into words).\n",
    "   - Convert all words to lowercase to ensure consistency.\n",
    "   - Optionally, remove stop words (e.g., \"the,\" \"is,\" \"and\") and perform stemming or lemmatization.\n",
    "\n",
    "2. **Vocabulary Creation**:\n",
    "   - Create a vocabulary of unique words from the corpus (all text data).\n",
    "   - Assign an index to each unique word.\n",
    "\n",
    "3. **Encoding**:\n",
    "   - Represent each document (text) as a vector, where:\n",
    "     - Each element corresponds to a word in the vocabulary.\n",
    "     - The value is the frequency (or presence/absence) of the word in the document.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "#### Input:\n",
    "```text\n",
    "Document 1: \"I like apples\"\n",
    "Document 2: \"I like bananas\"\n",
    "Document 3: \"I eat apples and bananas\"\n",
    "```\n",
    "\n",
    "#### Step 1: Create Vocabulary\n",
    "Vocabulary: `['i', 'like', 'apples', 'bananas', 'eat', 'and']`\n",
    "\n",
    "#### Step 2: Encode Documents as Vectors\n",
    "Each document is converted into a vector based on word counts:\n",
    "\n",
    "| Word       | `I` | `like` | `apples` | `bananas` | `eat` | `and` |\n",
    "|------------|-----|--------|----------|-----------|-------|-------|\n",
    "| Document 1 | 1   | 1      | 1        | 0         | 0     | 0     |\n",
    "| Document 2 | 1   | 1      | 0        | 1         | 0     | 0     |\n",
    "| Document 3 | 1   | 0      | 1        | 1         | 1     | 1     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7e4fdce5-a32f-4caf-ab98-17ab2d2811ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'apples' 'bananas' 'eat' 'like']\n",
      "Bag of Words Matrix:\n",
      " [[0 1 0 0 1]\n",
      " [0 0 1 0 1]\n",
      " [1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample data\n",
    "documents = [\n",
    "    \"I like apples\",\n",
    "    \"I like bananas\",\n",
    "    \"I eat apples and bananas\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Display results\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "53ae5486-e393-4707-9287-515e62e6157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['apples' 'bananas' 'eat' 'like']\n",
      "Bag of Words Matrix:\n",
      " [[1 0 0 1]\n",
      " [0 1 0 1]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')  # Remove common stop words\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad7291-da12-4e17-a498-39b191a71a99",
   "metadata": {},
   "source": [
    "### **Advantages of CoutnVectorizer**\n",
    "1. **Automatic Text Processing**: Automates tokenization, vocabulary creation, and counting.\n",
    "2. **Flexible**: Allows customization with parameters like stop word removal and n-grams.\n",
    "3. **Integration**: Easy to integrate with machine learning models in scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages of CoutnVectorizer**\n",
    "1. **No Context**: Ignores word order and semantic meaning.\n",
    "2. **High Dimensionality**: For large vocabularies, results in sparse and high-dimensional matrices.\n",
    "3. **Sensitive to Rare Words**: Rare words may disproportionately affect the representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc15bd-384c-4706-bf0b-34d1a871c3b3",
   "metadata": {},
   "source": [
    "### **Advantages of Bag of Words**\n",
    "1. **Simple and Intuitive**: Easy to implement and understand.\n",
    "2. **Works Well for Small Datasets**: Effective for small-scale text classification or clustering tasks.\n",
    "3. **Foundation for Other Models**: Basis for more advanced techniques like TF-IDF and word embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages of Bag of Words**\n",
    "1. **High Dimensionality**:\n",
    "   - For a large vocabulary, the resulting vectors are high-dimensional and sparse.\n",
    "   - This increases memory and computational requirements.\n",
    "\n",
    "2. **No Context or Order Information**:\n",
    "   - Ignores the sequence of words.\n",
    "   - Loses the semantic meaning of words and phrases.\n",
    "\n",
    "3. **Ignores Synonyms and Polysemy**:\n",
    "   - Treats synonyms as separate words (e.g., \"happy\" and \"joyful\").\n",
    "   - The same word used in different contexts has the same representation (e.g., \"bank\" in \"river bank\" vs. \"money bank\").\n",
    "\n",
    "4. **Sparse Representation**:\n",
    "   - Vectors are filled mostly with zeros, making them inefficient to process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5411582-615f-46e6-b87d-ffa279451e47",
   "metadata": {},
   "source": [
    "## 10. TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c407f87-9db5-4dd6-9956-5b2920291070",
   "metadata": {},
   "source": [
    "**TF-IDF** is a statistical method used in Natural Language Processing (NLP) to evaluate the importance of a word in a document relative to a collection of documents (corpus). Unlike **Bag of Words (BoW)**, which counts word occurrences, TF-IDF assigns weights to words based on their frequency in a single document and their rarity across all documents, helping to identify key terms.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components**\n",
    "1. **Term Frequency (TF)**:\n",
    "   - Measures how often a word occurs in a document.\n",
    "   - Formula:\n",
    "     \\[\n",
    "     TF = Number of occurrences of the word in the document \\ Total words in the document\n",
    "     \\]\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**:\n",
    "   - Measures how unique or rare a word is across all documents.\n",
    "   - Formula:\n",
    "     \\[\n",
    "     IDF = log(Total number of documents \\ Number of documents containing the word)\n",
    "     \\]\n",
    "   - A word appearing in many documents has a low IDF value, while a rare word has a high IDF value.\n",
    "\n",
    "3. **TF-IDF Score**:\n",
    "   - Combines TF and IDF to calculate the importance of a word in a document.\n",
    "   - Formula:\n",
    "     \\[\n",
    "     TF-IDF = TF* IDF\n",
    "     \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "#### Documents:\n",
    "- Document 1: \"I love apples\"\n",
    "- Document 2: \"I love bananas\"\n",
    "- Document 3: \"Apples and bananas are great\"\n",
    "\n",
    "#### Vocabulary:\n",
    "`['i', 'love', 'apples', 'bananas', 'and', 'are', 'great']`\n",
    "\n",
    "#### Calculating TF-IDF:\n",
    "For the word **\"apples\"**:\n",
    "- **TF (Document 1)**: 1\\3 = 0.33\n",
    "- **IDF**: log(3/2) = 0.18\n",
    "- **TF-IDF (Document 1)**: 0.33 * 0.18 = 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8d7aabc9-e35f-48e1-a205-c4106448b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      " [[0.         0.70710678 0.         0.         0.         0.70710678]\n",
      " [0.         0.         0.         0.70710678 0.         0.70710678]\n",
      " [0.49047908 0.37302199 0.49047908 0.37302199 0.49047908 0.        ]]\n",
      "Vocabulary: ['and' 'apples' 'are' 'bananas' 'great' 'love']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample data\n",
    "documents = [\n",
    "    \"I love apples\",\n",
    "    \"I love bananas\",\n",
    "    \"Apples and bananas are great\"\n",
    "]\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array for viewing\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631af23-ed82-4598-b725-5e826e26bcce",
   "metadata": {},
   "source": [
    "### **Advantages of TF-IDF**\n",
    "1. **Distinguishes Important Words**:\n",
    "   - Identifies words that are significant in a document but uncommon in the corpus.\n",
    "2. **Simple and Effective**:\n",
    "   - Works well for small to medium-sized datasets.\n",
    "3. **Reduces Noise**:\n",
    "   - Reduces the weight of common but uninformative words.\n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages of TF-IDF**\n",
    "1. **No Contextual Understanding**:\n",
    "   - Fails to capture word meanings and relationships.\n",
    "2. **Sensitive to Data Sparsity**:\n",
    "   - High-dimensional representations can still be sparse for large corpora.\n",
    "3. **Static Weights**:\n",
    "   - Weights are fixed after computation and don't adapt to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7157474-0d09-4b32-a17f-511aad2b1f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
